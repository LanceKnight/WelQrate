{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02:23:19] Initializing Normalizer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from thermo import functional_groups\n",
    "from Bio import Entrez\n",
    "from chembl_structure_pipeline import checker\n",
    "\n",
    "from rdkit.Chem import rdMolDescriptors, Descriptors, Lipinski, Crippen, inchi\n",
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogParams\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from data_gathering import download_and_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/AID463087' # Name your data folder\n",
    "dataset_name = 'AID463087_Cav3_inhibitor' # Name your dataset\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing data, need to identify which AIDs will be included. Data will be imported from https://pubchem.ncbi.nlm.nih.gov/assay/pcget. For more information on PubChem's programmatic access, refer to: https://pubchem.ncbi.nlm.nih.gov/docs/bioassays. Some other programmatic access options available such as PUG-REST. However, these might not be optimal for bulk retrieval or handling of large dataset due to the limitation of request volume. \n",
    "\n",
    "Data for individual assays include 7 required columns (CIDs, isomeric SMILES, etc.) and optional test results. Refer to https://ftp.ncbi.nlm.nih.gov/pubchem/Bioassay/CSV/README for further details. For datasets intended for regression model, additional columns could be extracted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets retrieving:  6\n"
     ]
    }
   ],
   "source": [
    "# Desired AIDs:\n",
    "AIDs = [449739, 493021, 489005, 493022, 493023, 493041]\n",
    "\n",
    "#Keep unique values in list AIDs (since there could be overlapping AIDs from different targets or project)\n",
    "AIDs = list(set(AIDs))\n",
    "AIDs = [str(AID) for AID in AIDs]\n",
    "print('Number of datasets retrieving: ', len(AIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 6 complete\n",
      "2 out of 6 complete\n",
      "3 out of 6 complete\n",
      "4 out of 6 complete\n",
      "5 out of 6 complete\n",
      "6 out of 6 complete\n"
     ]
    }
   ],
   "source": [
    "#Data to be extracted from the assay:\n",
    "col_list = ['PUBCHEM_CID','PUBCHEM_EXT_DATASOURCE_SMILES', 'PUBCHEM_ACTIVITY_OUTCOME']\n",
    "download_and_save(AIDs, data_folder, col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Isomeric SMILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of our project, we would like to include isomeric form of SMILES representation in our final dataset. Although PubChem claimed that their datatable should include isomeric SMILES (https://pubchem.ncbi.nlm.nih.gov/docs/bioassays), some dataset might include non-isomeric SMILES. This step is to import isomeric SMILES based on CIDs. \n",
    "\n",
    "Several packages such as RDkit have modules to return isomeric SMILES from a given input SMILES. However, for consistency, we decided to use the PubChem Identifier Exchange Service, which take an input identifier (CIDs, SMILES, InChI, etc.)  and return the corresponding identifier (CIDs, isomeric SMILES, InChIs, etc.). Here, we export the list of CIDs for compounds in our dataset and use this server to retrieve their isomeric SMILES. For more information, refer to: https://pubchem.ncbi.nlm.nih.gov/docs/identifier-exchange-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAfter this step, submit the lists of CIDs at https://pubchem.ncbi.nlm.nih.gov/idexchange/idexchange.cgi \\n    Operator type: \"same CID\" \\n    Output IDs \"SMILES\" (isomeric SMILES by default) \\n    Output method: \"Two column file showing each input output-correspondence\"\\n    Compression: \"No compression\"\\nRefer to https://pubchem.ncbi.nlm.nih.gov/docs/identifier-exchange-service for more details.\\nThe output files (converted isomeric SMILES) should be named as \"isomeric_smi_{AID}.txt\" and saved to the \"step_2\" folder.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new step folder:\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_2'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_2')\n",
    "\n",
    "#Export list of CIDs to csv with one column without the column name:\n",
    "for AID in AIDs:\n",
    "    assay = pd.read_csv(f'{data_folder}/before_finished/step_1/AID{AID}.csv')\n",
    "    assay['PUBCHEM_CID'].to_csv(f'{data_folder}/before_finished/step_2/CID{AID}.csv', index=False, header=False)\n",
    "    \n",
    "\"\"\"\n",
    "After this step, submit the lists of CIDs at https://pubchem.ncbi.nlm.nih.gov/idexchange/idexchange.cgi \n",
    "    Operator type: \"same CID\" \n",
    "    Output IDs \"SMILES\" (isomeric SMILES by default) \n",
    "    Output method: \"Two column file showing each input output-correspondence\"\n",
    "    Compression: \"No compression\"\n",
    "Refer to https://pubchem.ncbi.nlm.nih.gov/docs/identifier-exchange-service for more details.\n",
    "The output files (converted isomeric SMILES) should be named as \"isomeric_smi_{AID}.txt\" and saved to the \"step_2\" folder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_isomeric_smiles(AIDs):\n",
    "    \"\"\"\n",
    "    Check if the SMILES in the assay are isomeric or not.\n",
    "    Input: AIDs (list of strings)\n",
    "    Output: non_isomeric_smi_cids (dictionary with AID as key and list of non-isomeric CIDs as values for the datasets in AIDs\n",
    "    \"\"\"\n",
    "    non_isomeric_smi_cids = {}\n",
    "    for AID in AIDs:    \n",
    "        non_isomeric_smi_cids[AID] = []\n",
    "        #import SMILES.txt file as a table:\n",
    "        correct_isomeric_smiles = pd.read_csv(f'{data_folder}/before_finished/step_2/isomeric_smi_{AID}.txt', sep='\\t', header=None)\n",
    "        assay = pd.read_csv(f'{data_folder}/before_finished/step_1/AID{AID}.csv')\n",
    "\n",
    "        #compare smiles in assay with smiles in correct_smiles:\n",
    "        for cid in assay['PUBCHEM_CID']:\n",
    "            if assay.loc[assay['PUBCHEM_CID'] == cid, 'PUBCHEM_EXT_DATASOURCE_SMILES'].values[0] != correct_isomeric_smiles.loc[correct_isomeric_smiles[0] == cid, 1].values[0]:\n",
    "                non_isomeric_smi_cids[AID].append(cid)\n",
    "\n",
    "        if len(non_isomeric_smi_cids[AID]) == 0:\n",
    "            print(f'All SMILES in AID {AID} are isomeric')\n",
    "        else:\n",
    "            print(f'There are some non-isomeric SMILES in AID {AID}:')\n",
    "            print(non_isomeric_smi_cids[AID])\n",
    "\n",
    "    return non_isomeric_smi_cids\n",
    "\n",
    "def update_isomeric(AIDs, non_isomeric_smi_cids):\n",
    "    \"\"\"\n",
    "    Update the SMILES in the assay to isomeric SMILES.\n",
    "    Input: AIDs (list of strings), non_isomeric_smi_cids (dictionary with AID as key and list of non-isomeric CIDs as values)\n",
    "    \"\"\"\n",
    "    with open(f'{data_folder}/before_finished/step_2/non_isomeric_smi_cids.txt', 'w') as f:\n",
    "        # record the non-isomeric SMILES \n",
    "        for AID in AIDs:\n",
    "            assay = pd.read_csv(f'{data_folder}/before_finished/step_1/AID{AID}.csv')\n",
    "            correct_isomeric_smiles = pd.read_csv(f'{data_folder}/before_finished/step_2/isomeric_smi_{AID}.txt', sep='\\t', header=None)\n",
    "            f.write(f'AID {AID}: {non_isomeric_smi_cids[AID]}\\n')\n",
    "\n",
    "            for cid in non_isomeric_smi_cids[AID]:\n",
    "                f.write(f'CID {cid}: {assay.loc[assay[\"PUBCHEM_CID\"] == cid, \"PUBCHEM_EXT_DATASOURCE_SMILES\"].values[0]} -> {correct_isomeric_smiles.loc[correct_isomeric_smiles[0] == cid, 1].values[0]}\\n')\n",
    "                assay.loc[assay['PUBCHEM_CID'] == cid, 'PUBCHEM_EXT_DATASOURCE_SMILES'] = correct_isomeric_smiles.loc[correct_isomeric_smiles[0] == cid, 1].values[0]\n",
    "\n",
    "            f.write(f'===\\n')\n",
    "            assay.to_csv(f'{data_folder}/before_finished/step_2/AID{AID}.csv', index=False)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some non-isomeric SMILES in AID 449739:\n",
      "[2997662, 2997957, 2999888]\n",
      "All SMILES in AID 489005 are isomeric\n",
      "All SMILES in AID 493041 are isomeric\n",
      "All SMILES in AID 493021 are isomeric\n",
      "All SMILES in AID 493022 are isomeric\n",
      "All SMILES in AID 493023 are isomeric\n"
     ]
    }
   ],
   "source": [
    "non_isomeric_smi_cids = check_isomeric_smiles(AIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here they returned that three smiles in AID449739 were not isomeric. This shows that the SMILES representation of some compounds in the given datasets might not be isomeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_isomeric(AIDs, non_isomeric_smi_cids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. InChI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to include standard InChI to diversify users' choice of which data they would like to use for their own benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new step folder:\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_3'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is convenient to use the PubChem Identifier Exchange Service (https://pubchem.ncbi.nlm.nih.gov/idexchange/idexchange.cgi) with operator type \"same CID\" and Output IDs \"InChI\" to retrieve InChI from a given list of input CIDs. The same CID lists from STEP 2 could be used here. The resulted InChIs could be checked if being standard by indentifying the presence of 'InChI=1S' at the begining of each InChI string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CID lists (in \"step_2\" folder should be submitted to PubChem Identifier Exchange Service)\n",
    "    Operator type: \"same CID\" \n",
    "    Output IDs \"InChI\"\n",
    "    Output method: \"Two column file showing each input output-correspondence\"\n",
    "    Compression: \"No compression\"\n",
    "InChI list should be saved into \"step_3\" folder, named as \"std_inchi_{AID}.txt\" \n",
    "\"\"\"\n",
    "# Import dataframes:\n",
    "for AID in AIDs: \n",
    "    exec(f'AID{AID} = pd.read_csv(\"{data_folder}/before_finished/step_2/AID{AID}.csv\")')\n",
    "    exec(f'AID{AID}_InChI = pd.read_csv(\"{data_folder}/before_finished/step_3/std_inchi_{AID}.txt\", sep=\"\\\\t\", header=None)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All InChI in AID449739 are standard\n",
      "All InChI in AID489005 are standard\n",
      "All InChI in AID493041 are standard\n",
      "All InChI in AID493021 are standard\n",
      "All InChI in AID493022 are standard\n",
      "All InChI in AID493023 are standard\n"
     ]
    }
   ],
   "source": [
    "#Check if they are all standard InChI:\n",
    "for AID in AIDs:\n",
    "    check_inchi = f\"\"\"\n",
    "non_standard_InChI = []\n",
    "for i in range(len(AID{AID}_InChI[1])):\n",
    "    if not AID{AID}_InChI[1][i].startswith('InChI=1S'):\n",
    "        non_standard_InChI.append(AID{AID}_InChI[1][i])\n",
    "if not non_standard_InChI:\n",
    "    print('All InChI in AID{AID} are standard')\n",
    "else:\n",
    "    print('There are some non-standard InChI in AID{AID}')\n",
    "    print(non_standard_InChI)\n",
    "    print('===')\n",
    "\"\"\"\n",
    "    exec(check_inchi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we concatenate the InChIs in our tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update and save the files\n",
    "for AID in AIDs: \n",
    "    update_inchi = f\"\"\"\n",
    "AID{AID}_InChI_dict = dict(zip(AID{AID}_InChI[0], AID{AID}_InChI[1]))\n",
    "AID{AID}['InChI'] = AID{AID}['PUBCHEM_CID'].map(AID{AID}_InChI_dict)\n",
    "AID{AID}['InChI'] = AID{AID}['InChI'].astype(str)\n",
    "AID{AID}.to_csv(r\"{data_folder}/before_finished/step_3/AID{AID}.csv\", index=False)\n",
    "\"\"\"\n",
    "    exec(update_inchi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Check duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking duplicates in the datasets, we would like to know if there are\n",
    "1) Multiple identical molecules\n",
    "2) Molecules with identical CID but different InChIs or SMILES\n",
    "3) Molecules with identical InChI but with different CIDs or SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import:\n",
    "for AID in AIDs:\n",
    "    exec(f\"AID{AID} = pd.read_csv(r'{data_folder}/before_finished/step_3/AID{AID}.csv', sep=',', header=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new step folder:\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_4'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Checking identical molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of AID449739 InChI duplicates:  40\n",
      "Number of AID449739 SMILES duplicates:  28\n",
      "Number of AID449739 CID duplicates:  28\n",
      "Number of AID489005 InChI duplicates:  0\n",
      "Number of AID489005 SMILES duplicates:  0\n",
      "Number of AID489005 CID duplicates:  0\n",
      "Number of AID493041 InChI duplicates:  0\n",
      "Number of AID493041 SMILES duplicates:  0\n",
      "Number of AID493041 CID duplicates:  0\n",
      "Number of AID493021 InChI duplicates:  0\n",
      "Number of AID493021 SMILES duplicates:  0\n",
      "Number of AID493021 CID duplicates:  0\n",
      "Number of AID493022 InChI duplicates:  0\n",
      "Number of AID493022 SMILES duplicates:  0\n",
      "Number of AID493022 CID duplicates:  0\n",
      "Number of AID493023 InChI duplicates:  0\n",
      "Number of AID493023 SMILES duplicates:  0\n",
      "Number of AID493023 CID duplicates:  0\n"
     ]
    }
   ],
   "source": [
    "#Return all duplicates by comparing InChI, SMILES, and CIDs:\n",
    "for AID in AIDs:\n",
    "    check_duplicate = f\"\"\"\n",
    "AID{AID}_duplicates_InChI = AID{AID}[AID{AID}.duplicated(subset=['InChI'], keep=False)]\n",
    "AID{AID}_duplicates_SMILES = AID{AID}[AID{AID}.duplicated(subset=['PUBCHEM_EXT_DATASOURCE_SMILES'], keep=False)]\n",
    "AID{AID}_duplicates_CIDs = AID{AID}[AID{AID}.duplicated(subset=['PUBCHEM_CID'], keep=False)]\n",
    "print('Number of AID{AID} InChI duplicates: ', len(AID{AID}_duplicates_InChI))\n",
    "print('Number of AID{AID} SMILES duplicates: ', len(AID{AID}_duplicates_SMILES))\n",
    "print('Number of AID{AID} CID duplicates: ', len(AID{AID}_duplicates_CIDs))\n",
    "\"\"\"\n",
    "    exec(check_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write duplicates to a txt file: \n",
    "with open(f'{data_folder}/before_finished/step_4/duplicates.txt', 'w') as f:\n",
    "    for AID in AIDs: \n",
    "        duplicates_InChI = eval(f'AID{AID}_duplicates_InChI')\n",
    "        duplicates_SMILES = eval(f'AID{AID}_duplicates_SMILES')\n",
    "        duplicates_CIDs = eval(f'AID{AID}_duplicates_CIDs')\n",
    "        f.write(f'\\n\\nAID{AID} InChI duplicates:\\n')\n",
    "        f.write(duplicates_InChI.to_string())\n",
    "        f.write(f'\\nAID{AID} SMILES duplicates:\\n')\n",
    "        f.write(duplicates_SMILES.to_string())\n",
    "        f.write(f'\\nAID{AID} CID duplicates:\\n')\n",
    "        f.write(duplicates_CIDs.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Same CIDs but different chemical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex\n",
    "for AID in AIDs: \n",
    "    exec(f\"AID{AID}_duplicates_CIDs.reset_index(drop=True, inplace=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_folder}/before_finished/step_4/sameCID_different_others.txt', 'w') as f:\n",
    "    for AID in AIDs: \n",
    "        sameCID_differentInChI = []\n",
    "        sameCID_differentSMILES = []\n",
    "        duplicates_CIDs = eval(f'AID{AID}_duplicates_CIDs')\n",
    "        for i in range(len(duplicates_CIDs['PUBCHEM_CID'])):\n",
    "            for j in range(i+1, len(duplicates_CIDs['PUBCHEM_CID'])):\n",
    "                if duplicates_CIDs['PUBCHEM_CID'][i] == duplicates_CIDs['PUBCHEM_CID'][j]:\n",
    "                    if duplicates_CIDs['InChI'][i] != duplicates_CIDs['InChI'][j]:\n",
    "                        sameCID_differentInChI.append((duplicates_CIDs['PUBCHEM_CID'][i], duplicates_CIDs['PUBCHEM_CID'][j]))\n",
    "                    if duplicates_CIDs['PUBCHEM_EXT_DATASOURCE_SMILES'][i] != duplicates_CIDs['PUBCHEM_EXT_DATASOURCE_SMILES'][j]:\n",
    "                        sameCID_differentSMILES.append((duplicates_CIDs['PUBCHEM_CID'][i], duplicates_CIDs['PUBCHEM_CID'][j]))\n",
    "\n",
    "        if sameCID_differentInChI == []:\n",
    "            f.write(f'No duplicate CIDs with different InChIs in AID{AID}\\n')\n",
    "        else:\n",
    "            f.write('Found duplicate CIDs with different InChIs in AID{AID}:\\n')\n",
    "            f.write('\\n'.join(str(item) for item in sameCID_differentInChI))\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if sameCID_differentSMILES == []:\n",
    "            f.write(f'No duplicate CIDs with different SMILES in AID{AID}\\n')\n",
    "        else:\n",
    "            f.write(f'Found duplicate CIDs with different SMILES in AID{AID}:\\n')\n",
    "            f.write('\\n'.join(str(item) for item in sameCID_differentSMILES))\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Same InChI but with different CIDs or SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex\n",
    "for AID in AIDs: \n",
    "    exec(f\"AID{AID}_duplicates_InChI.reset_index(drop=True, inplace=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_folder}/before_finished/step_4/sameInChI_different_others.txt', 'w') as f:\n",
    "    for AID in AIDs: \n",
    "        sameInChI_differentCID = []\n",
    "        sameInChI_differentSMILES = []\n",
    "        duplicates_InChI = eval(f'AID{AID}_duplicates_InChI')\n",
    "        for i in range(len(duplicates_InChI['InChI'])):\n",
    "            for j in range(i+1, len(duplicates_InChI['InChI'])):\n",
    "                if duplicates_InChI['InChI'][i] == duplicates_InChI['InChI'][j]:\n",
    "                    if duplicates_InChI['PUBCHEM_CID'][i] != duplicates_InChI['PUBCHEM_CID'][j]:\n",
    "                        sameInChI_differentCID.append((duplicates_InChI['PUBCHEM_CID'][i], duplicates_InChI['PUBCHEM_CID'][j]))\n",
    "                    if duplicates_InChI['PUBCHEM_EXT_DATASOURCE_SMILES'][i] != duplicates_InChI['PUBCHEM_EXT_DATASOURCE_SMILES'][j]:\n",
    "                        sameInChI_differentSMILES.append((duplicates_InChI['PUBCHEM_CID'][i], duplicates_InChI['PUBCHEM_CID'][j]))\n",
    "        \n",
    "        if sameInChI_differentCID == []:\n",
    "            f.write(f'No duplicate InChIs with different CIDs in AID{AID}\\n')\n",
    "        else:\n",
    "            f.write('Found duplicate InChIs with different CIDs in AID{AID}:\\n')\n",
    "            f.write('\\n'.join(str(item) for item in sameInChI_differentCID))\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if sameInChI_differentSMILES == []:\n",
    "            f.write(f'No duplicate InChIs with different SMILES in AID{AID}\\n')\n",
    "        else:\n",
    "            f.write(f'Found duplicate InChIs with different SMILES in AID{AID}:\\n')\n",
    "            f.write('\\n'.join(str(item) for item in sameInChI_differentSMILES))\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Same SMILES but with different CIDs or SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex\n",
    "for AID in AIDs: \n",
    "    exec(f\"AID{AID}_duplicates_SMILES.reset_index(drop=True, inplace=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_folder}/before_finished/step_4/sameSMILES_different_others.txt', 'w') as f:\n",
    "    for AID in AIDs: \n",
    "        sameSMILES_differentCID = []\n",
    "        sameSMILES_differentInChI = []\n",
    "        duplicates_SMILES = eval(f'AID{AID}_duplicates_SMILES')\n",
    "        for i in range(len(duplicates_SMILES['PUBCHEM_EXT_DATASOURCE_SMILES'])):\n",
    "            for j in range(i+1, len(duplicates_SMILES['PUBCHEM_EXT_DATASOURCE_SMILES'])):\n",
    "                if duplicates_SMILES['PUBCHEM_EXT_DATASOURCE_SMILES'][i] == duplicates_SMILES['PUBCHEM_EXT_DATASOURCE_SMILES'][j]:\n",
    "                    if duplicates_SMILES['PUBCHEM_CID'][i] != duplicates_SMILES['PUBCHEM_CID'][j]:\n",
    "                        sameSMILES_differentCID.append((duplicates_SMILES['PUBCHEM_CID'][i], duplicates_SMILES['PUBCHEM_CID'][j]))\n",
    "                    if duplicates_SMILES['InChI'][i] != duplicates_SMILES['InChI'][j]:\n",
    "                        sameSMILES_differentInChI.append((duplicates_SMILES['PUBCHEM_CID'][i], duplicates_SMILES['PUBCHEM_CID'][j]))\n",
    "        \n",
    "        if sameSMILES_differentCID == []:\n",
    "            f.write(f'No duplicate SMILES with different CIDs in AID{AID}\\n')\n",
    "        else:\n",
    "            f.write(f'Found duplicate SMILES with different CIDs in AID{AID}:\\n')\n",
    "            f.write('\\n'.join(str(item) for item in sameSMILES_differentCID))\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if sameSMILES_differentInChI == []:\n",
    "            f.write(f'No duplicate SMILES with different InChIs in AID{AID}\\n')\n",
    "        else:\n",
    "            f.write(f'Found duplicate SMILES with different InChIs in AID{AID}:\\n')\n",
    "            f.write('\\n'.join(str(item) for item in sameSMILES_differentInChI))\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Drop duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dropping duplicates, we will keep the first molecule in a pair or a group of duplicates. For example, here there are 12 duplicates (6 pairs) so we keep 6 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more duplicate InChI in AID449739\n",
      "No more duplicate InChI in AID489005\n",
      "No more duplicate InChI in AID493041\n",
      "No more duplicate InChI in AID493021\n",
      "No more duplicate InChI in AID493022\n",
      "No more duplicate InChI in AID493023\n"
     ]
    }
   ],
   "source": [
    "# Keep only the first duplicate in the dataframes:\n",
    "for AID in AIDs: \n",
    "    exec(f\"AID{AID}.drop_duplicates(subset=['InChI'], keep='first', inplace=True)\")\n",
    "\n",
    "    last_check = f\"\"\"\n",
    "if len(AID{AID}[AID{AID}.duplicated(subset=['InChI'], keep=False)]) == 0:\n",
    "    print('No more duplicate InChI in AID{AID}')\n",
    "else:\n",
    "    print('There are still duplicate InChI in AID{AID}')   \n",
    "    \"\"\"\n",
    "    exec(last_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more duplicate SMILES in AID449739\n",
      "No more duplicate SMILES in AID489005\n",
      "No more duplicate SMILES in AID493041\n",
      "No more duplicate SMILES in AID493021\n",
      "No more duplicate SMILES in AID493022\n",
      "No more duplicate SMILES in AID493023\n"
     ]
    }
   ],
   "source": [
    "for AID in AIDs: \n",
    "    last_check = f\"\"\"\n",
    "if len(AID{AID}[AID{AID}.duplicated(subset=['PUBCHEM_EXT_DATASOURCE_SMILES'], keep=False)]) == 0:\n",
    "    print('No more duplicate SMILES in AID{AID}')\n",
    "else:\n",
    "    print('There are still duplicate SMILES in AID{AID}')   \n",
    "    \"\"\"\n",
    "    exec(last_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more duplicate CID in AID449739\n",
      "No more duplicate CID in AID489005\n",
      "No more duplicate CID in AID493041\n",
      "No more duplicate CID in AID493021\n",
      "No more duplicate CID in AID493022\n",
      "No more duplicate CID in AID493023\n"
     ]
    }
   ],
   "source": [
    "for AID in AIDs: \n",
    "    last_check = f\"\"\"\n",
    "if len(AID{AID}[AID{AID}.duplicated(subset=['PUBCHEM_CID'], keep=False)]) == 0:\n",
    "    print('No more duplicate CID in AID{AID}')\n",
    "else:\n",
    "    print('There are still duplicate CID in AID{AID}')   \n",
    "    \"\"\"\n",
    "    exec(last_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframes to csv:\n",
    "for AID in AIDs: \n",
    "    exec(f\"AID{AID}.to_csv(r'{data_folder}/before_finished/step_4/AID{AID}.csv', index=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hierarchical Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hierarchical curation, there are some rules: \n",
    "\n",
    "(1) All assays used should be on the same or close species/cell lines. Optimally, they should also be from the same project/laboratory.\n",
    "\n",
    "(2) Primary actives (PrA) will have a large false-positive rate. Therefore, they should be tested in follow-up confirmatory screens (optimally dose-reponse). \n",
    "\n",
    "(3) Actives could be promiscuous. Therefore, it is optimal to have counter-screens on different targets to test specificity.\n",
    "\n",
    "(4) For some projects, compounds were tested in multiple rounds. Therefore, assays often have hierarchical relations. From a single primary screen (Pr), active compounds (Pr_A) could be tested in multiple rounds of confirmatory screens (Cf_1, Cf_2, ..., Cf_final) or counter screens (Ct_1, Ct_2, etc.). Actives from confirmatory screens (Cf_A) have a higher possibility of being true active. If an active compound is tested active in counter screens (Ct_A), it is likely to be a promiscuous compound and should not be included. \n",
    "\n",
    "(4) It is important to know the relationship between assays. Active sets from downstream screens always have a lower false-positive rate than active sets from upstream screens due to better assay technologies on a smaller set of compounds. Therefore, final hits should be taken from the intersection of the very last confirmatory assays, without tested active in any counter-screens: \n",
    "Final hits = [Cf_final1_A ∩ Cf_final2_A ∩ ...] \\ [Ct_1_A ∪ Ct_2_A ∪ ...]\n",
    "\n",
    "However, if the confirmatory assays are unrelated (tested on different set of compounds), then we might have to take the union of their active sets instead of the intersections as in this formula.\n",
    "\n",
    "(5) The hierarchical relations should be inspected carefully to see if follow-up confirmatory screens include extra compounds (Ex) that were not tested in earlier screens or tested inactive in earlier screens. If exist, these compounds require manual inspection. \n",
    "\n",
    "(6) Final inactives should be taken from primary inactives (Pr_I) (not inconclusive, unspecified, or probes), plus extra compounds that were tested inactive in conformatory screens (Ex_I), if justified.\n",
    "Final inactives = PrI ∪ Ex_I\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Classify groups of compounds in each assay by activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{data_folder}/before_finished/step_4' \n",
    "keynumbers = [449739, 493021, 489005, 493022, 493023, 493041] # specify the keynumbers you want to import\n",
    "\n",
    "for keynumber in keynumbers:\n",
    "    filename = os.path.join(path, f'AID{keynumber}.csv')\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        exec(f'AID{keynumber} = df')\n",
    "        exec(f'AID{keynumber}_active = df[df[\"PUBCHEM_ACTIVITY_OUTCOME\"]==\"Active\"]')\n",
    "        exec(f'AID{keynumber}_inactive = df[df[\"PUBCHEM_ACTIVITY_OUTCOME\"]==\"Inactive\"]')\n",
    "        exec(f'AID{keynumber}_inconclusive = df[df[\"PUBCHEM_ACTIVITY_OUTCOME\"]==\"Inconclusive\"]')\n",
    "        exec(f'AID{keynumber}_unspecified = df[df[\"PUBCHEM_ACTIVITY_OUTCOME\"]==\"Unspecified\"]')\n",
    "        exec(f'AID{keynumber}_probe = df[df[\"PUBCHEM_ACTIVITY_OUTCOME\"]==\"Probe\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AID</th>\n",
       "      <th>Tested Compounds</th>\n",
       "      <th>Active</th>\n",
       "      <th>Inactive</th>\n",
       "      <th>Inconclusive</th>\n",
       "      <th>Unspecified</th>\n",
       "      <th>Probe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AID449739</td>\n",
       "      <td>104722</td>\n",
       "      <td>4230</td>\n",
       "      <td>100492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AID493021</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AID489005</td>\n",
       "      <td>895</td>\n",
       "      <td>703</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AID493022</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AID493023</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AID493041</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AID  Tested Compounds  Active  Inactive  Inconclusive  Unspecified  \\\n",
       "0  AID449739            104722    4230    100492             0            0   \n",
       "1  AID493021                17      17         0             0            0   \n",
       "2  AID489005               895     703       192             0            0   \n",
       "3  AID493022                44      41         3             0            0   \n",
       "4  AID493023                29      25         4             0            0   \n",
       "5  AID493041                32      24         8             0            0   \n",
       "\n",
       "   Probe  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a df with first column the variables name, and the second column the number of rows:\n",
    "df = pd.DataFrame(columns=['AID', 'Tested Compounds', 'Active', 'Inactive', 'Inconclusive', 'Unspecified', 'Probe'])\n",
    "for keynumber in keynumbers:\n",
    "    exec(f'df.loc[len(df)] = [\"AID{keynumber}\", len(AID{keynumber}), len(AID{keynumber}_active), len(AID{keynumber}_inactive), len(AID{keynumber}_inconclusive), len(AID{keynumber}_unspecified), len(AID{keynumber}_probe)]')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Check the hierachical relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_is_in(downstream, upstream): \n",
    "    downstream_in_upstream = downstream[downstream['PUBCHEM_CID'].isin(upstream['PUBCHEM_CID'])]\n",
    "    downstream_notin_upstream = downstream[~downstream['PUBCHEM_CID'].isin(upstream['PUBCHEM_CID'])]\n",
    "    return downstream_in_upstream, downstream_notin_upstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow: AID449739 (Pr), AID493021 (Cf_1a), AID489005 (Cf_1b), AID493022 (Cf_2a), AID493023 (Cf_2b), AID493041 (Cf_2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among AID493021, 0 were tested inactive in AID449739. Among these, 0 became active\n",
      "Among AID493021, 0 were not tested in the AID449739. Among these, 0 became active\n",
      "Among AID489005, 0 were tested inactive in AID449739. Among these, 0 became active\n",
      "Among AID489005, 0 were not tested in the AID449739. Among these, 0 became active\n"
     ]
    }
   ],
   "source": [
    "# Check between AID493021 and AID449739\n",
    "a1, a2 = check_is_in(AID493021, AID449739_inactive)\n",
    "a3, a4 = check_is_in(a1, AID493021_active)\n",
    "a5, a6 = check_is_in(AID493021, AID449739)\n",
    "a7, a8 = check_is_in(a6, AID493021_active)\n",
    "print(f'Among AID493021, {len(a1)} were tested inactive in AID449739. Among these, {len(a3)} became active')\n",
    "print(f'Among AID493021, {len(a6)} were not tested in the AID449739. Among these, {len(a7)} became active')\n",
    "\n",
    "# Check between AID489005 and AID449739\n",
    "b1, b2 = check_is_in(AID489005, AID449739_inactive)\n",
    "b3, b4 = check_is_in(a1, AID489005_active)\n",
    "b5, b6 = check_is_in(AID489005, AID449739)\n",
    "b7, b8 = check_is_in(a6, AID489005_active)\n",
    "print(f'Among AID489005, {len(b1)} were tested inactive in AID449739. Among these, {len(b3)} became active')\n",
    "print(f'Among AID489005, {len(b6)} were not tested in the AID449739. Among these, {len(b7)} became active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among AID493022, 0 were tested inactive in AID449739. Among these, 0 became active\n",
      "Among AID493022, 44 were not tested in the AID449739. Among these, 41 became active\n"
     ]
    }
   ],
   "source": [
    "c1, c2 = check_is_in(AID493022, AID449739_inactive)\n",
    "c3, c4 = check_is_in(c1, AID493022_active)\n",
    "c5, c6 = check_is_in(AID493022, AID449739)\n",
    "c7, c8 = check_is_in(c6, AID493022_active)\n",
    "print(f'Among AID493022, {len(c1)} were tested inactive in AID449739. Among these, {len(c3)} became active')\n",
    "print(f'Among AID493022, {len(c6)} were not tested in the AID449739. Among these, {len(c7)} became active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among AID493023, 0 were tested inactive in AID449739. Among these, 0 became active\n",
      "Among AID493023, 29 were not tested in the AID449739. Among these, 25 became active\n"
     ]
    }
   ],
   "source": [
    "d1, d2 = check_is_in(AID493023, AID449739_inactive)\n",
    "d3, d4 = check_is_in(d1, AID493023_active)\n",
    "d5, d6 = check_is_in(AID493023, AID449739)\n",
    "d7, d8 = check_is_in(d6, AID493023_active)\n",
    "print(f'Among AID493023, {len(d1)} were tested inactive in AID449739. Among these, {len(d3)} became active')\n",
    "print(f'Among AID493023, {len(d6)} were not tested in the AID449739. Among these, {len(d7)} became active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among AID493041, 0 were tested inactive in AID449739. Among these, 0 became active\n",
      "Among AID493041, 32 were not tested in the AID449739. Among these, 24 became active\n"
     ]
    }
   ],
   "source": [
    "e1, e2 = check_is_in(AID493041, AID449739_inactive)\n",
    "e3, e4 = check_is_in(e1, AID493041_active)\n",
    "e5, e6 = check_is_in(AID493041, AID449739)\n",
    "e7, e8 = check_is_in(e6, AID493041_active)\n",
    "print(f'Among AID493041, {len(e1)} were tested inactive in AID449739. Among these, {len(e3)} became active')\n",
    "print(f'Among AID493041, {len(e6)} were not tested in the AID449739. Among these, {len(e7)} became active')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the two large confirmatory datasets AID493021 and AID489005 have no problem. None of the compounds tested in the rest three confirmatory assays (AID493022, AID493023, and AID493041) were not even tested from primary screen. The assays protocol showed that these are newly synthesized molecules similar to confirmed actives to test as potential hits.\n",
    "\n",
    "The potential final hits therefore should be any compound that were active in all confirmatory screens: AID493021, AID489005, AID493022, AID493023, and AID493041. Inactives should be taken from the primary inactives, plus some extra newly synthesized compounds that were confirmed to be inactive in the three small confirmatory screens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We end up with 791 potential hits.\n"
     ]
    }
   ],
   "source": [
    "# concatenate all the dataframes to confirmed hits: \n",
    "confirmed_hits = pd.concat([AID493021_active, AID489005_active, AID493022_active, AID493023_active, AID493041_active])\n",
    "confirmed_hits = confirmed_hits.drop_duplicates(subset=['PUBCHEM_CID'], keep='first')\n",
    "potential_hits = confirmed_hits\n",
    "print(f'We end up with {len(confirmed_hits)} potential hits.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We end up with 100507 potential inactives.\n"
     ]
    }
   ],
   "source": [
    "potential_inactives = pd.concat([AID449739_inactive, c8, d8, e8])\n",
    "potential_inactives = potential_inactives.drop_duplicates(subset=['PUBCHEM_CID'], keep='first')\n",
    "print(f'We end up with {len(potential_inactives)} potential inactives.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{data_folder}/before_finished/step_5'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the potential hits and inactives to csv:\n",
    "potential_hits.to_csv(f'{data_folder}/before_finished/step_5/potential_hits.csv', index=False)\n",
    "potential_inactives.to_csv(f'{data_folder}/before_finished/step_5/potential_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. RDkit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_hits = pd.read_csv(f'{data_folder}/before_finished/step_5/potential_hits.csv', sep=',', header=0)\n",
    "potential_inactives = pd.read_csv(f'{data_folder}/before_finished/step_5/potential_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smiles(df):\n",
    "    \"\"\"\n",
    "    This function check if the SMILES strings from a given dataset could be parsed by RDKit or if they returns any problems detected by RDkit\n",
    "    Input: \n",
    "        Pandas dataframe\n",
    "    Output: \n",
    "        mol_list: dictionary with CID as key and RDKit molecule object as value\n",
    "        problem_list: list of problems detected by RDKit\n",
    "        cannot_parse: list of CIDs that could not be parsed by RDKit\n",
    "    \"\"\"\n",
    "    mol_list = {}\n",
    "    problem_list = []\n",
    "    cannot_parse = []\n",
    "\n",
    "    for i in df['PUBCHEM_CID']:\n",
    "\n",
    "        #convert each SMILES to molecule:\n",
    "        m = Chem.MolFromSmiles(df[df['PUBCHEM_CID'] == i]['PUBCHEM_EXT_DATASOURCE_SMILES'].values[0], sanitize=True)\n",
    "        mol_list[i] = m\n",
    "\n",
    "        if m is None:\n",
    "            cannot_parse.append(i) #save if molecule is non-parsable\n",
    "            \n",
    "        elif m is not None:\n",
    "            problems = Chem.DetectChemistryProblems(m) #identify and capture error messages when creating mol objects.\n",
    "        if problems != ():\n",
    "            problem_list.append(problems)\n",
    "            \n",
    "    if len(problem_list) > 0: \n",
    "        print(problem_list)\n",
    "    else:\n",
    "        print(\"No problems detected\")\n",
    "    return mol_list, problem_list, cannot_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No problems detected\n",
      "No problems detected\n"
     ]
    }
   ],
   "source": [
    "mol_hits, problem_list_hits, cannot_parse_hits = process_smiles(potential_hits)\n",
    "mol_inactives, problem_list_inactives, cannot_parse_inactives = process_smiles(potential_inactives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new step folder:\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_6'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_6')\n",
    "\n",
    "with open(f'{data_folder}/before_finished/step_6/problem_list_hits.txt', 'w') as f:\n",
    "    f.write(\"Problems:\\n\")\n",
    "    for item in problem_list_hits:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    f.write(\"Cannot parse:\\n\")\n",
    "    for item in cannot_parse_hits:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(f'{data_folder}/before_finished/step_6/problem_list_inactives.txt', 'w') as f:\n",
    "    f.write(\"Problems:\\n\")\n",
    "    for item in problem_list_inactives:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    f.write(\"Cannot parse:\\n\")\n",
    "    for item in cannot_parse_inactives:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset returned no problem or non-parsable molecule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Inorganics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "potential_hits = pd.read_csv(f'{data_folder}/before_finished/step_5/potential_hits.csv', sep=',', header=0)\n",
    "potential_inactives = pd.read_csv(f'{data_folder}/before_finished/step_5/potential_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "organic_hits = []\n",
    "inorganic_hits = []\n",
    "\n",
    "for index, row in potential_hits.iterrows():\n",
    "    cid = row['PUBCHEM_CID']\n",
    "    smi = row['PUBCHEM_EXT_DATASOURCE_SMILES']\n",
    "    mol = Chem.MolFromSmiles(smi, sanitize=True)\n",
    "    if functional_groups.is_inorganic(mol):\n",
    "        inorganic_hits.append(cid)\n",
    "    else:\n",
    "        organic_hits.append(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "organic_inactives = []\n",
    "inorganic_inactives = []\n",
    "\n",
    "for index, row in potential_inactives.iterrows():\n",
    "    cid = row['PUBCHEM_CID']\n",
    "    smi = row['PUBCHEM_EXT_DATASOURCE_SMILES']\n",
    "    mol = Chem.MolFromSmiles(smi, sanitize=True)\n",
    "    if functional_groups.is_inorganic(mol):\n",
    "        inorganic_inactives.append(cid)\n",
    "    else: \n",
    "        organic_inactives.append(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In hits, there are 791 organic molecules and 0 inorganic molecules\n",
      "In inactives, there are 100507 organic molecules and 0 inorganic molecules\n"
     ]
    }
   ],
   "source": [
    "print(f'In hits, there are {len(organic_hits)} organic molecules and {len(inorganic_hits)} inorganic molecules')\n",
    "print(f'In inactives, there are {len(organic_inactives)} organic molecules and {len(inorganic_inactives)} inorganic molecules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new step folder:\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_7'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_7')\n",
    "\n",
    "with open(f'{data_folder}/before_finished/step_7/inorganic.txt', 'w') as f:\n",
    "    f.write(\"Hits:\\n\")\n",
    "    for item in inorganic_hits:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    f.write(\"\\n\\nInactives:\\n\")\n",
    "    for item in inorganic_inactives:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop inorganics: \n",
    "potential_hits = potential_hits[~potential_hits['PUBCHEM_CID'].isin(inorganic_hits)]\n",
    "potential_inactives = potential_inactives[~potential_inactives['PUBCHEM_CID'].isin(inorganic_inactives)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save: \n",
    "potential_hits.to_csv(f'{data_folder}/before_finished/step_7/organic_hits.csv', index=False)\n",
    "potential_inactives.to_csv(f'{data_folder}/before_finished/step_7/organic_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import: \n",
    "organic_hits = pd.read_csv(f'{data_folder}/before_finished/step_7/organic_hits.csv', sep=',', header=0)\n",
    "organic_inactives = pd.read_csv(f'{data_folder}/before_finished/step_7/organic_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mixtures in hits is 27\n",
      "Total number of mixtures in inactives is 3659\n"
     ]
    }
   ],
   "source": [
    "def quick_check_mixtures(name, smiles_list):\n",
    "    count=0\n",
    "    for smiles in smiles_list:\n",
    "        if '.' in smiles:\n",
    "            count+=1\n",
    "    print(f\"Total number of mixtures in {name} is {count}\")\n",
    "\n",
    "quick_check_mixtures('hits', organic_hits['PUBCHEM_EXT_DATASOURCE_SMILES'])\n",
    "quick_check_mixtures('inactives', organic_inactives['PUBCHEM_EXT_DATASOURCE_SMILES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Handling mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smiles_dataframe(df):\n",
    "    \"\"\"\n",
    "    From a given dataframe, detect and handle mixtures based on SMILES representation.\n",
    "    Input: \n",
    "        Pandas dataframe\n",
    "    Output: \n",
    "        (dictionary: CID -> SMILES)\n",
    "            processed: non-mixture forms of every SMILES in the given dataset\n",
    "            removed: mixture components or mixtures removed from the original mixture SMILES\n",
    "            small_organic: small organic molecules that are removed from a size-imbalanced mixtures of organic molecules\n",
    "            small_inorganic: small inorganic molecules that are removed from a mixture of both organic and inorganic molecules\n",
    "            big_organic_not_lipinski: large organic molecules that are not kept due to not passing the lipinski criteria\n",
    "            cleaned_from_mixtures: non-mixture forms after handling of the orignal mixtures\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    removed = {}\n",
    "    small_inorganic = {}\n",
    "    small_organic = {}\n",
    "    big_organic_not_lipinski = {}\n",
    "    cleaned_from_mixtures = {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        cid = row['PUBCHEM_CID']\n",
    "        smiles = row['PUBCHEM_EXT_DATASOURCE_SMILES']\n",
    "        \n",
    "        if '.' in smiles: # Check for mixtures\n",
    "            molecules = smiles.split('.')\n",
    "            mols = [Chem.MolFromSmiles(mol) for mol in molecules]\n",
    "            num_atoms = [mol.GetNumAtoms() for mol in mols if mol is not None]\n",
    "            \n",
    "            if all(x == num_atoms[0] for x in num_atoms): # Check if all molecules have the same number of atoms. If yes, keep one of them\n",
    "                processed[cid] = molecules[0]\n",
    "                cleaned_from_mixtures[cid] = molecules[0]\n",
    "                removed[cid] = '.'.join(molecules[1:])\n",
    "            else:\n",
    "                if max(num_atoms) - min(num_atoms) <= 5:\n",
    "                    removed[cid] = smiles # Remove the mixture if the difference in number of atoms is less than 5\n",
    "                    print(f\"Cannot decide between {molecules} for CID {cid}\")\n",
    "                else:\n",
    "                    max_index = num_atoms.index(max(num_atoms))\n",
    "                    min_index = num_atoms.index(min(num_atoms))\n",
    "                    if functional_groups.is_inorganic(mols[min_index]) == True:\n",
    "                        processed[cid] = molecules[max_index]\n",
    "                        cleaned_from_mixtures[cid] = molecules[max_index]\n",
    "                        removed[cid] = molecules[min_index] # Keep the organic molecule and remove the inorganic one\n",
    "                        small_inorganic[cid] = molecules[min_index]\n",
    "                    else:\n",
    "                        big_molecule = mols[max_index]\n",
    "                        \n",
    "                        # Calculate properties for Lipinski's rule of five\n",
    "                        mw = Descriptors.MolWt(big_molecule)\n",
    "                        hbd = rdMolDescriptors.CalcNumHBD(big_molecule)\n",
    "                        hba = rdMolDescriptors.CalcNumHBA(big_molecule)\n",
    "                        logp = Crippen.MolLogP(big_molecule)\n",
    "\n",
    "                        # Check Lipinski's criteria\n",
    "                        if mw <= 500 and hbd <= 5 and hba <= 10 and logp <= 5:\n",
    "                            processed[cid] = molecules[max_index] # Keep the big organic molecule if it passes Lipinski's rule of five\n",
    "                            cleaned_from_mixtures[cid] = molecules[max_index]\n",
    "                            removed[cid] = '.'.join([molecules[i] for i in range(len(molecules)) if i != max_index])\n",
    "                            small_organic[cid] = molecules[min_index]\n",
    "                        else:\n",
    "                            removed[cid] = smiles # Remove the mixture if the big organic molecule does not pass Lipinski's rule of five\n",
    "                            big_organic_not_lipinski[cid] = molecules[max_index]\n",
    "                            print(f\"Big organic molecule for CID {cid} does not pass Lipinski's rule of five\")\n",
    "\n",
    "        else:\n",
    "            processed[cid] = smiles\n",
    "            \n",
    "    return processed, removed, small_organic, small_inorganic, big_organic_not_lipinski, cleaned_from_mixtures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot decide between ['CN(C)C1=NC2=CC=CC=C2C(=C1)N', 'C1=C(NC(=O)NC1=O)C(=O)O'] for CID 646688\n",
      "Cannot decide between ['C1C2(CN3CN1CN(C2)C3)N', 'C1=CC(=CN=C1)C(=O)O'] for CID 648270\n",
      "Cannot decide between ['CN(C)C1=NC2=C(CCC2)C(=C1)N', 'C1=C(C=NC=C1O)C(=O)O'] for CID 652177\n",
      "Cannot decide between ['CC1=NC2=C(S1)C3=CC=CC=C3C=C2', 'C1=C(C=C(C(=C1[N+](=O)[O-])O)[N+](=O)[O-])[N+](=O)[O-]'] for CID 654127\n",
      "Big organic molecule for CID 657180 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['C1CC1C(C2CC2)NC3=NCCO3', 'C(=C/C(=O)O)\\\\C(=O)O'] for CID 5388964\n",
      "Cannot decide between ['CN1C(=O)C2=C(N=C(N2)Cl)N(C1=O)C(=O)[O-]', 'CN(C)CCOC(C1=CC=CC=C1)C2=CC=CC=C2'] for CID 657227\n",
      "Cannot decide between ['CC[N+](C)(C)CC1=CC=CC=C1Br', 'CC1=CC=C(C=C1)S(=O)(=O)[O-]'] for CID 6100\n",
      "Cannot decide between ['C1CN(CCN1CCOCCO)C(C2=CC=CC=C2)C3=CC=C(C=C3)Cl', 'C1=CC=C2C(=C1)C=C(C(=C2CC3=C(C(=CC4=CC=CC=C43)C(=O)O)O)O)C(=O)O'] for CID 25096\n",
      "Big organic molecule for CID 5284439 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['CC1=NC2=C(O1)C3=CC=CC=C3C=C2', 'C1=C(C=C(C(=C1[N+](=O)[O-])O)[N+](=O)[O-])[N+](=O)[O-]'] for CID 3241713\n",
      "Big organic molecule for CID 3244813 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 8566 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 227456 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['CNC[C@@H](C1=CC(=C(C=C1)O)O)O', 'C(C(C(=O)O)O)(C(=O)O)O'] for CID 6852374\n",
      "Cannot decide between ['C1CCC(C(C1)N)N', 'Cl[Pt]Cl'] for CID 151689\n",
      "Big organic molecule for CID 6420009 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['CCN(CC)C(=O)N1CCN(CC1)C', 'C(C(=O)O)C(CC(=O)O)(C(=O)O)O'] for CID 15432\n",
      "Cannot decide between ['CC1=C(SC=C1)/C=C/C2=NCCCN2C', 'C(C(C(=O)O)O)(C(=O)O)O'] for CID 6419965\n",
      "Big organic molecule for CID 62881 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['CC(C)NC[C@H](C1=CC(=C(C=C1)O)O)O', 'C(C(C(=O)O)O)(C(=O)O)O'] for CID 6852409\n",
      "Cannot decide between ['C(CS)N', 'Cl'] for CID 9082\n",
      "Big organic molecule for CID 9549148 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['CC1=NC(CC2=CC=CC=C12)(C)C', 'C1=CC=C(C(=C1)C(=O)O)O'] for CID 9549466\n",
      "Big organic molecule for CID 9549634 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 9549642 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['CCCC1=NCCN2C1=CC=C2', 'C(=C/C(=O)O)\\\\C(=O)O'] for CID 5717182\n",
      "Big organic molecule for CID 135512395 does not pass Lipinski's rule of five\n",
      "Cannot decide between ['CSC1=NC=C(C(=N1)C(=O)[O-])Cl', 'C1=CC=C(C=C1)CC[NH3+]'] for CID 9552033\n",
      "Big organic molecule for CID 11948864 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 135490316 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 5729990 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 135712454 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 16219228 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 9896684 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 23723054 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 23581804 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 3005572 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 9847786 does not pass Lipinski's rule of five\n",
      "Big organic molecule for CID 23581803 does not pass Lipinski's rule of five\n"
     ]
    }
   ],
   "source": [
    "processed_hits, removed_hits, small_organic_hits, small_inorganic_hits, not_lipinski_hits, cleaned_hits = process_smiles_dataframe(organic_hits)\n",
    "processed_inactives, removed_inactives, small_organic_inactives, small_inorganic_inactives, not_lipinski_inactives, cleaned_inactives = process_smiles_dataframe(organic_inactives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new step folder\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_8'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate df with the smiles column in the cleaned_hits or cleaned_inactives dictionary:\n",
    "cleaned_hits_df = pd.DataFrame(list(cleaned_hits.values()), columns=['PUBCHEM_EXT_DATASOURCE_SMILES'])\n",
    "cleaned_inactives_df = pd.DataFrame(list(cleaned_inactives.values()), columns=['PUBCHEM_EXT_DATASOURCE_SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the cleaned hits and inactives to csv:\n",
    "cleaned_hits_df.to_csv(f'{data_folder}/before_finished/step_8/cleaned_mixtures_hits.csv', index=False, header=False)\n",
    "cleaned_inactives_df.to_csv(f'{data_folder}/before_finished/step_8/cleaned_mixtures_inactives.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646688 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "648270 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "652177 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "654127 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "657180 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "5388964 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "657227 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "6100 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "25096 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "5284439 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "3241713 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "3244813 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "8566 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "227456 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "6852374 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "151689 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "6420009 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "15432 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "6419965 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "62881 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "6852409 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9082 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9549148 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9549466 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9549634 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9549642 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "5717182 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "135512395 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9552033 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "11948864 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "135490316 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "5729990 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "135712454 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "16219228 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9896684 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "23723054 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "23581804 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "3005572 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "9847786 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n",
      "23581803 has been removed from inactives_M1_antagonist because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\n"
     ]
    }
   ],
   "source": [
    "def process_mixture_df(name, df, processed, removed, small_organic, small_inorganic):\n",
    "    \"\"\"\n",
    "    Update a given dataframe with information on mixture handling\n",
    "    \"\"\"\n",
    "    indices_to_drop = []  # List to keep track of row indices that should be dropped\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        cid = row['PUBCHEM_CID']\n",
    "        if cid in processed:\n",
    "            df.loc[index, 'PUBCHEM_EXT_DATASOURCE_SMILES'] = processed[cid]\n",
    "            if cid in removed:\n",
    "                df.loc[index, 'Mol removed from mixture'] = removed[cid]\n",
    "            if cid in small_organic:\n",
    "                df.loc[index, 'Small organic molecule'] = small_organic[cid]\n",
    "            if cid in small_inorganic:\n",
    "                df.loc[index, 'Small inorganic molecule'] = small_inorganic[cid]\n",
    "        else:\n",
    "            indices_to_drop.append(index)\n",
    "            print(f\"{cid} has been removed from {name} because it is a mixture with less than 5 atoms difference or the big organic molecule does not pass Lipinski's rule of five.\")\n",
    "    \n",
    "    # Drop rows outside the loop and reset index if needed\n",
    "    new_df = df.drop(indices_to_drop).reset_index(drop=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "processed_hits_df = process_mixture_df('hits_M1_antagonist', organic_hits, processed_hits, removed_hits, small_organic_hits, small_inorganic_hits)\n",
    "processed_inactives_df = process_mixture_df('inactives_M1_antagonist', organic_inactives, processed_inactives, removed_inactives, small_organic_inactives, small_inorganic_inactives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes saved successfully\n"
     ]
    }
   ],
   "source": [
    "with open(f'{data_folder}/before_finished/step_8/mixture.txt', 'w') as f:\n",
    "    f.write(f\"\"\"\n",
    "Hits before processing: {len(organic_hits)}\n",
    "Hits before processing: {len(organic_hits)}\n",
    "Hits after processing: {len(processed_hits_df)}\n",
    "Mixtures detected: {len(removed_hits)}\n",
    "Mixtures with small inorganic molecules: {len(small_inorganic_hits)}\n",
    "Mixtures with big organic molecules passing Lipinski: {len(small_organic_hits)}\n",
    "Mixtures with big organic molecules not passing Lipinski: {len(not_lipinski_hits)}\n",
    "\n",
    "Inactives before processing: {len(organic_inactives)}\n",
    "Inactives after processing: {len(processed_inactives_df)}\n",
    "Mixtures detected: {len(removed_inactives)}\n",
    "Mixtures with small inorganic molecules: {len(small_inorganic_inactives)}\n",
    "Mixtures with big organic molecules passing Lipinski: {len(small_organic_inactives)}\n",
    "Mixtures with big organic molecules not passing Lipinski: {len(not_lipinski_inactives)}\n",
    "\"\"\")\n",
    "\n",
    "# Save the processed dataframes to csv\n",
    "processed_hits_df.to_csv(f'{data_folder}/before_finished/step_8/post8_hits.csv', index=False)\n",
    "processed_inactives_df.to_csv(f'{data_folder}/before_finished/step_8/post8_inactives.csv', index=False)\n",
    "\n",
    "print('Dataframes saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Neutralize & 10. Aromatize molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new step folder:\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_9_10'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_9_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import:\n",
    "pre9_hits = pd.read_csv(f'{data_folder}/before_finished/step_8/post8_hits.csv', sep=',', header=0)\n",
    "pre9_inactives = pd.read_csv(f'{data_folder}/before_finished/step_8/post8_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize_atoms(mol):\n",
    "    \"\"\"\n",
    "    Code adapted from https://www.rdkit.org/docs/Cookbook.html. \n",
    "    Source: https://baoilleach.blogspot.com/2019/12/no-charge-simple-approach-to.html\n",
    "    (Noel O’Boyle, 2019)\n",
    "\n",
    "    This function return a neutralized molecules for a given input Mol object. \n",
    "    Additional handling was added for molecules with tetracoordinated boron. \n",
    "    \"\"\"\n",
    "    pattern = Chem.MolFromSmarts(\"[+1!h0!$([*]~[-1,-2,-3,-4]),-1!$([*]~[+1,+2,+3,+4])]\")\n",
    "    at_matches = mol.GetSubstructMatches(pattern)\n",
    "    at_matches_list = [y[0] for y in at_matches]\n",
    "    if len(at_matches_list) > 0:\n",
    "        for at_idx in at_matches_list:\n",
    "            atom = mol.GetAtomWithIdx(at_idx)\n",
    "            chg = atom.GetFormalCharge()\n",
    "            hcount = atom.GetTotalNumHs()\n",
    "            \n",
    "            #Skip adjustment for tetracoordinated boron\n",
    "            if atom.GetAtomicNum() == 5 and atom.GetDegree() == 4: #ADD COMMENT\n",
    "                continue  # Just bypass the problematic atom\n",
    "\n",
    "            atom.SetFormalCharge(0)\n",
    "            atom.SetNumExplicitHs(hcount - chg)\n",
    "            atom.UpdatePropertyCache()\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aromatize_smile(mol):\n",
    "    \"\"\"\n",
    "    This function dekekulize an input Mol object and return the aromatic form of isomeric SMILES. \n",
    "    \"\"\"\n",
    "    Chem.Kekulize(mol)\n",
    "    Chem.SanitizeMol(mol, Chem.SanitizeFlags.SANITIZE_ALL)\n",
    "    aromatic_smiles = Chem.MolToSmiles(mol, isomericSmiles = True)\n",
    "    return aromatic_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_smi = []\n",
    "\n",
    "#Update dataset with neutralized, aromatic SMILES\n",
    "for smi in pre9_hits['PUBCHEM_EXT_DATASOURCE_SMILES']: \n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    mol_neu = neutralize_atoms(mol)\n",
    "    smi_arom = aromatize_smile(mol_neu)\n",
    "    updated_smi.append(smi_arom)\n",
    "    \n",
    "#update the smiles in this df\n",
    "pre9_hits['PUBCHEM_EXT_DATASOURCE_SMILES'] = updated_smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_smi = []\n",
    "\n",
    "#Update dataset with neutralized, aromatic SMILES\n",
    "for smi in pre9_inactives['PUBCHEM_EXT_DATASOURCE_SMILES']: \n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    mol_neu = neutralize_atoms(mol)\n",
    "    smi_arom = aromatize_smile(mol_neu)\n",
    "    updated_smi.append(smi_arom)\n",
    "\n",
    "#update the smiles in this df\n",
    "pre9_inactives['PUBCHEM_EXT_DATASOURCE_SMILES'] = updated_smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "pre9_hits.to_csv(f'{data_folder}/before_finished/step_9_10/post10_hits.csv', index=False)\n",
    "pre9_inactives.to_csv(f'{data_folder}/before_finished/step_9_10/post10_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post 9+10: Update InChI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it important to now update InChI in our datasets, for 2 reasons:\n",
    "\n",
    "(1) Some mixture compounds have been modified (removal of small inorganic or organic molecules) in SMILES representation but not InChIs.\n",
    "\n",
    "(2) The SMILES representations have been neutralized and aromatized, but not InChIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the smiles columns to txt\n",
    "pre9_hits['PUBCHEM_EXT_DATASOURCE_SMILES'].to_csv(f'{data_folder}/before_finished/step_9_10/smiles_hits.txt', index=False, header=False)\n",
    "pre9_inactives['PUBCHEM_EXT_DATASOURCE_SMILES'].to_csv(f'{data_folder}/before_finished/step_9_10/smiles_inactives.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Submit the smiles files to PubChem Identifier Exchange Service: \n",
    "    Input IDs: \"SMILES\"\n",
    "    Operator type: \"same CID\" \n",
    "    Output IDs: \"InChI\"\n",
    "    Output method: \"Two column file showing each input output-correspondence\"\n",
    "    Compression: \"No compression\"\n",
    "InChI list should be saved into \"step_9_10\" folder, named as \"inchi_hits.txt\" and \"inchi_inactives\" \n",
    "\"\"\"\n",
    "#Import the converted InChIs\n",
    "cleaned_inchi_hits = pd.read_csv(f'{data_folder}/before_finished/step_9_10/inchi_hits.txt', sep='\\t', header=None)\n",
    "cleaned_inchi_inactives = pd.read_csv(f'{data_folder}/before_finished/step_9_10/inchi_inactives.txt', sep='\\t', header=None)\n",
    "\n",
    "#a dictionary of smiles and corresponding inchi in cleaned_inchi_hits\n",
    "hits_smi_inchi_dict = dict(zip(cleaned_inchi_hits[0], cleaned_inchi_hits[1]))\n",
    "inactives_smi_inchi_dict = dict(zip(cleaned_inchi_inactives[0], cleaned_inchi_inactives[1]))\n",
    "                             \n",
    "#update the pre9_hits by matching the smiles with keys and replace inchi with values:\n",
    "pre9_hits['InChI'] = pre9_hits['PUBCHEM_EXT_DATASOURCE_SMILES'].map(hits_smi_inchi_dict) \n",
    "pre9_inactives['InChI'] = pre9_inactives['PUBCHEM_EXT_DATASOURCE_SMILES'].map(inactives_smi_inchi_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export: \n",
    "pre9_hits.to_csv(f'{data_folder}/before_finished/step_9_10/post10_hits.csv', index=False)\n",
    "pre9_inactives.to_csv(f'{data_folder}/before_finished/step_9_10/post10_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. PAIN filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. Frequency of hits (FoH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of Hits is a complex concept that requires a merticulous approach. In general, the rule is if a compound was tested active in multiple assays, it is likely to be a promiscuous compound. \n",
    "1. For each compounds, retrieve the information on its tested assays\n",
    "2. For each of the assay tested, retrieve the sequence of the protein target. \n",
    "3. Given all sequence of the protein tested, do a multiple sequence alignment to find the percentage Percent Identity (similarty) between these proteins. If an assay has high percentage to other targets, then these assays contribute less to promiscuousity of the compound. \n",
    "4. Use the percentage identity as a weight: \n",
    "w = 1 - %SI/100\n",
    "Calculate the frequency of hits for each compound:\n",
    "FoH = wACC/TAC\n",
    "wACC is the weighed total number of assay tested where the compounds were identified acitives. TAC is the total number of assays tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre11_hits = pd.read_csv(f'{data_folder}/before_finished/step_9_10/post10_hits.csv', sep=',', header=0)\n",
    "pre11_inactives = pd.read_csv(f'{data_folder}/before_finished/step_9_10/post10_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new step folder:\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_11/11_1'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_11/11_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1. PubChem testing information for each compound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part illustrates how to retrieve the information of how each compound was tested from the PubChem database. Bulk data retrieval from the ftp server is used to get the information of every bioassay in PubChem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to H:/coding/HiChem/curation/pubchem_sum\\bioassays.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ftp.ncbi.nlm.nih.gov/pubchem/Bioassay/Extras/bioassays.tsv.gz' #this FTP file records the summary data of all available AIDs in PubChem\n",
    "\n",
    "local_save_dir = 'H:/coding/HiChem/curation/pubchem_sum'\n",
    "local_save_path = os.path.join(local_save_dir, 'bioassays.tsv.gz')\n",
    "\n",
    "if not os.path.exists(local_save_dir):\n",
    "    os.makedirs(local_save_dir)\n",
    "r = requests.get(url, stream=True)\n",
    "\n",
    "with open(local_save_path, 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=8192):\n",
    "        f.write(chunk)\n",
    "print('Downloaded to %s' % local_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'pubchem_sum/bioassays.tsv.gz'\n",
    "\n",
    "# Read the TSV file\n",
    "all_bioassay = pd.read_csv(path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AID</th>\n",
       "      <th>BioAssay Name</th>\n",
       "      <th>Deposit Date</th>\n",
       "      <th>Modify Date</th>\n",
       "      <th>Source Name</th>\n",
       "      <th>Source ID</th>\n",
       "      <th>Substance Type</th>\n",
       "      <th>Outcome Type</th>\n",
       "      <th>Project Category</th>\n",
       "      <th>BioAssay Group</th>\n",
       "      <th>BioAssay Types</th>\n",
       "      <th>Protein Accessions</th>\n",
       "      <th>UniProts IDs</th>\n",
       "      <th>Gene IDs</th>\n",
       "      <th>Target TaxIDs</th>\n",
       "      <th>Taxonomy IDs</th>\n",
       "      <th>Number of Tested SIDs</th>\n",
       "      <th>Number of Active SIDs</th>\n",
       "      <th>Number of Tested CIDs</th>\n",
       "      <th>Number of Active CIDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>20040815</td>\n",
       "      <td>20240410</td>\n",
       "      <td>DTP/NCI</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>small-molecule</td>\n",
       "      <td>Confirmatory</td>\n",
       "      <td>Other</td>\n",
       "      <td>NCI-60_DOSERESP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55228</td>\n",
       "      <td>3318</td>\n",
       "      <td>53214</td>\n",
       "      <td>3094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>20040815</td>\n",
       "      <td>20240410</td>\n",
       "      <td>DTP/NCI</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>small-molecule</td>\n",
       "      <td>Confirmatory</td>\n",
       "      <td>Other</td>\n",
       "      <td>NCI-60_DOSERESP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51435</td>\n",
       "      <td>2615</td>\n",
       "      <td>49564</td>\n",
       "      <td>2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>20040815</td>\n",
       "      <td>20240410</td>\n",
       "      <td>DTP/NCI</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>small-molecule</td>\n",
       "      <td>Confirmatory</td>\n",
       "      <td>Other</td>\n",
       "      <td>NCI-60_DOSERESP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54079</td>\n",
       "      <td>2503</td>\n",
       "      <td>52046</td>\n",
       "      <td>2317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>20040815</td>\n",
       "      <td>20240410</td>\n",
       "      <td>DTP/NCI</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>small-molecule</td>\n",
       "      <td>Confirmatory</td>\n",
       "      <td>Other</td>\n",
       "      <td>NCI-60_DOSERESP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54062</td>\n",
       "      <td>4335</td>\n",
       "      <td>52033</td>\n",
       "      <td>4098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>20040815</td>\n",
       "      <td>20240410</td>\n",
       "      <td>DTP/NCI</td>\n",
       "      <td>NCI human tumor cell line growth inhibition as...</td>\n",
       "      <td>small-molecule</td>\n",
       "      <td>Confirmatory</td>\n",
       "      <td>Other</td>\n",
       "      <td>NCI-60_DOSERESP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53977</td>\n",
       "      <td>3159</td>\n",
       "      <td>52001</td>\n",
       "      <td>2981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AID                                      BioAssay Name  Deposit Date  \\\n",
       "0    1  NCI human tumor cell line growth inhibition as...      20040815   \n",
       "1    3  NCI human tumor cell line growth inhibition as...      20040815   \n",
       "2    5  NCI human tumor cell line growth inhibition as...      20040815   \n",
       "3    7  NCI human tumor cell line growth inhibition as...      20040815   \n",
       "4    9  NCI human tumor cell line growth inhibition as...      20040815   \n",
       "\n",
       "   Modify Date Source Name                                          Source ID  \\\n",
       "0     20240410     DTP/NCI  NCI human tumor cell line growth inhibition as...   \n",
       "1     20240410     DTP/NCI  NCI human tumor cell line growth inhibition as...   \n",
       "2     20240410     DTP/NCI  NCI human tumor cell line growth inhibition as...   \n",
       "3     20240410     DTP/NCI  NCI human tumor cell line growth inhibition as...   \n",
       "4     20240410     DTP/NCI  NCI human tumor cell line growth inhibition as...   \n",
       "\n",
       "   Substance Type  Outcome Type Project Category   BioAssay Group  \\\n",
       "0  small-molecule  Confirmatory            Other  NCI-60_DOSERESP   \n",
       "1  small-molecule  Confirmatory            Other  NCI-60_DOSERESP   \n",
       "2  small-molecule  Confirmatory            Other  NCI-60_DOSERESP   \n",
       "3  small-molecule  Confirmatory            Other  NCI-60_DOSERESP   \n",
       "4  small-molecule  Confirmatory            Other  NCI-60_DOSERESP   \n",
       "\n",
       "  BioAssay Types Protein Accessions UniProts IDs Gene IDs  Target TaxIDs  \\\n",
       "0            NaN                NaN          NaN      NaN            NaN   \n",
       "1            NaN                NaN          NaN      NaN            NaN   \n",
       "2            NaN                NaN          NaN      NaN            NaN   \n",
       "3            NaN                NaN          NaN      NaN            NaN   \n",
       "4            NaN                NaN          NaN      NaN            NaN   \n",
       "\n",
       "  Taxonomy IDs  Number of Tested SIDs  Number of Active SIDs  \\\n",
       "0          NaN                  55228                   3318   \n",
       "1          NaN                  51435                   2615   \n",
       "2          NaN                  54079                   2503   \n",
       "3          NaN                  54062                   4335   \n",
       "4          NaN                  53977                   3159   \n",
       "\n",
       "   Number of Tested CIDs  Number of Active CIDs  \n",
       "0                  53214                   3094  \n",
       "1                  49564                   2467  \n",
       "2                  52046                   2317  \n",
       "3                  52033                   4098  \n",
       "4                  52001                   2981  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bioassay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2 Retrieving protein sequences for assays tested:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the testing information for each compound is retrieved from the PugREST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache to store the number of compounds tested per AID to avoid redundant call. \n",
    "num_compounds_tested_cache = {}\n",
    "\n",
    "def get_num_compounds_tested(aid, all_bioassay=all_bioassay):\n",
    "    \"\"\"\n",
    "    This function retrieves the information of how many compounds were tested in a given assay (by AID).\n",
    "    \"\"\"\n",
    "    if aid in num_compounds_tested_cache:\n",
    "        return num_compounds_tested_cache[aid]\n",
    "    else: \n",
    "        #return the 'Number of Tested CIDs' column value at the row where the 'AID' column is equal to aid in the all_bioassay dataframe\n",
    "        num_compounds_tested = all_bioassay[all_bioassay['AID'] == aid]['Number of Tested CIDs'].values[0]\n",
    "    return num_compounds_tested\n",
    "\n",
    "def get_assay_data(cid):\n",
    "    \"\"\"\n",
    "    Return a dictionary of all targets that a given compound (by CID) was tested on in PubChem \n",
    "    and the activity values of the compound. \n",
    "    \"\"\"\n",
    "    url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/assaysummary/JSON\" #PUG-REST compound summary by CID\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    target_activity = {}\n",
    "\n",
    "    if 'Table' in data and 'Row' in data['Table']:\n",
    "        for row in data['Table']['Row']:\n",
    "            cells = row['Cell']\n",
    "            aid = int(cells[0])  # Extracting the AID from the first cell\n",
    "\n",
    "            # Proceed only if the assay is a screening assay\n",
    "            if cells[10] == 'Screening':\n",
    "\n",
    "                # Proceed only if more than 10,000 compounds were tested\n",
    "                num_compounds_tested = get_num_compounds_tested(aid)\n",
    "                if num_compounds_tested > 10000:\n",
    "                    target_gi = cells[5] # Retrieve the protein target's GI\n",
    "                    activity_outcome = cells[4].lower()\n",
    "\n",
    "                    if target_gi not in target_activity:\n",
    "                        target_activity[target_gi] = activity_outcome == 'active'\n",
    "                    elif activity_outcome == 'active':\n",
    "                        target_activity[target_gi] = True # If a compound was tested multiple times on the same protein, priotize \"active\" outcome.\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return (cid, target_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CIDs:   0%|          | 0/791 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CIDs: 100%|██████████| 791/791 [07:28<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "cids_list = pre11_hits['PUBCHEM_CID'].tolist()\n",
    "\n",
    "def execute_with_multiprocessing(cids_list):\n",
    "    \"\"\"\n",
    "    For a given list of CIDs, return a dictionary of dictionaries \n",
    "    of protein targets these compounds were tested on and the activity outcomes\n",
    "    Input: \n",
    "        [list of CIDs]\n",
    "    Output: \n",
    "        Dictionary of testing information for all CIDs, such as:\n",
    "        {CID1:{target1:activity1, target3:activity3, ...},{CID2:{target2:activity2, target4:activity4, ...}, ...}}\n",
    "    \"\"\"\n",
    "    results_dict = {}\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Prepare futures for all CIDs\n",
    "        futures = [executor.submit(get_assay_data, cid) for cid in cids_list]\n",
    "        \n",
    "        # Process futures as they complete\n",
    "        for future in tqdm(as_completed(futures), total=len(cids_list), desc=\"Processing CIDs\"):\n",
    "            try:\n",
    "                cid, target_activity = future.result()\n",
    "                results_dict[cid] = target_activity\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing CID: {e}\")\n",
    "    return results_dict\n",
    "\n",
    "results_dict = execute_with_multiprocessing(cids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export results_dict\n",
    "with open(f'{data_folder}/before_finished/step_11/11_1/results_dict.json', 'w') as f:\n",
    "    json.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import results_dict:\n",
    "with open(f'{data_folder}/before_finished/step_11/11_1/results_dict.json', 'r') as f:\n",
    "    results_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['222080095', '62868213', '4758878', '2182777540', '67463989', '2578455', '13325293', '4505209', '56202836', '1655766739', '730163', '6755076', '4581413', '194068499', '6680530', '5016090', '4506113', '499328', '16130726', '4502003', '2507196', '262118306', '83627717', '68565218', '89348172', '38156699', '68565074', '9966877', '119607129', '1679362728', '223460826', '148378801', '27894344', '301171662', '38349113', '4503385', '130375', '5454102', '83699673', '118764400', '76496497', '27368096', '81899072', '257380', '83779224', '15675770', '4503779', '12381848', '134244587', '23505220', '147728', '59036749', '156104889', '154146191', '16128424', '14149746', '42741659', '351542238', '13699818', '34577122', '7381449', '55976631', '47123300', '1572493', '225543099', '46909587', '15610807', '5729858', '14389423', '7108463', '47496637', '195969650', '8574038', '41872631', '85666113', '56790945', '180352', '21489979', '12830367', '534286618', '22538455', '4503351', '21361340', '115496662', '15610601', '55960760', '155969707', '109633019', '38258652', '68989256', '27807367', '29788785', '378544807', '208342286', '116734717', '119579178', '15927174', '253722402', '28949057', '82503229', '4507593', '13399304', '55584151', '73745819', '38788193', '5032039', '2853980', '881546', '21618340', '735367775', '998701', '70832125', '160877737', '1575471', '30219', '487738', '1797100823', '14790033', '116907', '112822', '121945198', '48428097', '1782953264', '78486550', '41872583', '21327705', '1927', '219518789', '4758208', '780303193', '5174513', '312275222', '124486680', '56417702', '28373018', '1519312078', '10864009', '15929025', '23893668', '4757840', '1708272', '124513266', '75495260', '124263658', '190938', '4507681', '339641', '115430235', '9629363', '6912644', '1781172', '5730106', '49168602', '86990435', '4505447', '32479527', '120538355', '15645703', '90111653', '341916350', '4506243', '32425330', '4503219', '109637798', '6009644', '433552101', '42794767', '90421313', '398366139', '218931251', '4503907', '166209887', '7657508', '116076351', '62201602', '4504843', '7582271', '160333370', '119622516', '32307126', '19860819', '27753985', '4507791', '1730321', '11275980', '7108336', '148745659', '62526033', '15680217', '56786138', '2498404', '317373446', '13236497', '120997', '117940060', '16306916', '40254439', '4507615', '14719829', '302699239', '32307152', '119508433', '164058', '49574532', '994798', '31542303', '119603173', '119580345', '32400300', '54112388', '194306653', '27597073', '21361095', '6708281', '124487323', '2393947', '48255881', '2935630', '19923198', '134142337', '4758484', '71746704', '486173', '7669492', '47132585', '124376142', '11093520', '21359873', '115347926', '285814664', '160707929', '16130689', '20336315', '47132611', '124809506', '1237937630', '1762973', '1628587', '10567816', '510901', '86301151', '4507793', '21595776', '16130724', '493539358', '342179211', '32400299', '113121', '126698238', '20070193', '45269145', '153217451', '63477962', '23943882', '38174238', '6274552', '37589898', '597517618', '23110962', '89191863', '13272532', '31563518', '110611243', '83318444', '21264324', '4504343', '134304838', '216409728', '55956923', '37187860', '13177715', '62740231', '151101270', '291463269', '6679827', '14790119', '13128862', '168184763', '74356043', '6016094', '55958172', '20072248', '71987181', '7706645', '17391426', '23510348', '156416009', '6323930', '111034851', '216548193', '11141885', '270133071', '23893623', '4502495', '62203298', '119579215', '10835145', '6166485', '83758679', '224494019', '7706135', '46367787', '37622910', '285809906', '62362414', '21595511', '60391226', '10092597', '1654220559', '231632', '116077694', '126642418', '21464101', '187960042', '223468676', '48145933', '125541954', '1246761', '88501734', '21955158', '119607128', '20336229', '9955963', '68476498', '28373962', '12803275', '67463988', '73747889', '76364066', '21315078', '115529463', '4758204', '4506537', '224028257', '929524245', '55662034', '46577642', '2702319', '7657550', '4826834', '68474550', '118341367', '71774083', '613504304', '54112432', '31881630', '12644416', '4826706', '4502331', '223459640', '3183518', '139472804', '296080766', '597517265', '1166512', '22035600', '40807040', '74355113', '48146199', '16130723', '4503383', '116292172', '17507875', '578162', '10835013', '67191027', '139424501', '124809271', '4502169', '53832009', '528078313', '4506055', '63102437', '216548487', '38027923', '10190672', '25148072', '149631', '11528014', '536029', '88702791', '74315350', '15610945', '167013344', '148539876', '21392848', '15609874', '1709543', '166202459', '4757950', '6831552', '5453722', '1302091', '24119166', '15646160', '2501205', '218891639', '73586699', '78070770', '296434520', '6978787', '187952397', '15610402', '9937384', '16878311', '47678551', '1937369734', '13124881', '11094021', '13027636', '4505445', '15607504', '111305821', '90652859', '74752344', '9629361', '4885057', '2358024', '34330186', '171229', '6325022', '52426748', '74734243', '1111959238', '116516899', '4503155', '89993689']\n",
      "Require multiple sequencing alignment for 427 proteins.\n"
     ]
    }
   ],
   "source": [
    "#get the list of all keys of the values in the dictionary:\n",
    "protein_ids = []\n",
    "for value in results_dict.values():\n",
    "    protein_ids.extend(value.keys())\n",
    "\n",
    "#clean the list\n",
    "protein_ids = list(set(protein_ids))\n",
    "protein_ids = [id for id in protein_ids if id != '']\n",
    "\n",
    "print(protein_ids)\n",
    "print(f'Require multiple sequencing alignment for {len(protein_ids)} proteins.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we retrieve all the FASTA sequences of proteins tested for all of our compounds with Biopython API to Entrez of NCBI. The FASTA sequence is saved as \"sequences.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always tell NCBI who you are\n",
    "Entrez.email = \"hdong26@amherst.edu\"\n",
    "\n",
    "# The filename where you want to save the sequences\n",
    "output_filename = f'{data_folder}/before_finished/step_11/11_1/sequences.fasta'\n",
    "\n",
    "# Open a file to write the sequences\n",
    "with open(output_filename, \"w\") as output_file:\n",
    "    for id in protein_ids:\n",
    "        try:\n",
    "            # Fetch the sequence from NCBI\n",
    "            handle = Entrez.efetch(db=\"protein\", id=id, rettype=\"fasta\", retmode=\"text\")\n",
    "            sequence_data = handle.read()\n",
    "            handle.close()\n",
    "            \n",
    "            # Write the sequence data to the file\n",
    "            output_file.write(sequence_data)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching {id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the FASTA sequence, we also need to retrieve the list of protein names, since these are different from the protein GIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protein_names(file_path):\n",
    "    protein_names = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                # Split the line at spaces and take the first item\n",
    "                parts = line.split(' ')\n",
    "                protein_name = parts[0]\n",
    "                # Remove the leading '>' character\n",
    "                protein_name = protein_name[1:]\n",
    "                protein_names.append(protein_name)\n",
    "    return protein_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{data_folder}/before_finished/step_11/11_1/sequences.fasta'\n",
    "protein_names = extract_protein_names(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map protein IDs to protein names by index \n",
    "protein_id_to_name = {protein_ids[i]: protein_names[i] for i in range(len(protein_ids))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 Percent Sequence Identity by Multiple Sequence Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences.fasta file is submitted to https://www.ebi.ac.uk/jdispatcher/msa/clustalo for multiple sequencing alignment. The resulted table of percent sequence identity matrix is saved and imported for the calculation of FoH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Submit sequences.fasta to https://www.ebi.ac.uk/jdispatcher/msa/clustalo\n",
    "    Input sequence type: Protein\n",
    "    Output format: ClustalW with character counts\n",
    "Download the resulted Percent Identity Matrix file file and save as \"percent_identity_matrix.txt\"\n",
    "\"\"\"\n",
    "\n",
    "#import the identity matrix:\n",
    "protein_si = pd.read_csv(\n",
    "    f'{data_folder}/before_finished/step_11/11_1/percent_identity_matrix.txt',\n",
    "    delimiter='\\s+',\n",
    "    header=None,\n",
    "    skiprows=6 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the first column:\n",
    "protein_si = protein_si.drop(protein_si.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = protein_si[1].tolist()\n",
    "name = ['protein name'] + name\n",
    "protein_si.columns = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein name</th>\n",
       "      <th>AAH65243.1</th>\n",
       "      <th>NP_937802.1</th>\n",
       "      <th>NP_001341533.1</th>\n",
       "      <th>AAG29340.1</th>\n",
       "      <th>NP_057685.1</th>\n",
       "      <th>NP_000469.3</th>\n",
       "      <th>NP_001074551.1</th>\n",
       "      <th>AAI32679.1</th>\n",
       "      <th>NP_776412.2</th>\n",
       "      <th>...</th>\n",
       "      <th>pdb|1VRU|A</th>\n",
       "      <th>sp|Q9HAT2.1|SIAE_HUMAN</th>\n",
       "      <th>EAN77629.1</th>\n",
       "      <th>NP_644805.1</th>\n",
       "      <th>NP_009330.1</th>\n",
       "      <th>NP_269944.1</th>\n",
       "      <th>NP_003896.1</th>\n",
       "      <th>AAH10859.1</th>\n",
       "      <th>NP_004070.3</th>\n",
       "      <th>AAA25265.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAH65243.1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>97.34</td>\n",
       "      <td>97.37</td>\n",
       "      <td>13.42</td>\n",
       "      <td>13.01</td>\n",
       "      <td>10.14</td>\n",
       "      <td>14.69</td>\n",
       "      <td>17.48</td>\n",
       "      <td>16.78</td>\n",
       "      <td>...</td>\n",
       "      <td>11.61</td>\n",
       "      <td>11.21</td>\n",
       "      <td>5.21</td>\n",
       "      <td>9.09</td>\n",
       "      <td>8.33</td>\n",
       "      <td>14.08</td>\n",
       "      <td>6.10</td>\n",
       "      <td>9.86</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP_937802.1</td>\n",
       "      <td>97.34</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>14.10</td>\n",
       "      <td>13.73</td>\n",
       "      <td>9.74</td>\n",
       "      <td>14.09</td>\n",
       "      <td>16.78</td>\n",
       "      <td>16.11</td>\n",
       "      <td>...</td>\n",
       "      <td>11.30</td>\n",
       "      <td>10.34</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.50</td>\n",
       "      <td>10.17</td>\n",
       "      <td>14.77</td>\n",
       "      <td>6.73</td>\n",
       "      <td>8.42</td>\n",
       "      <td>11.11</td>\n",
       "      <td>14.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP_001341533.1</td>\n",
       "      <td>97.37</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>14.20</td>\n",
       "      <td>13.84</td>\n",
       "      <td>9.74</td>\n",
       "      <td>14.09</td>\n",
       "      <td>16.78</td>\n",
       "      <td>16.11</td>\n",
       "      <td>...</td>\n",
       "      <td>11.30</td>\n",
       "      <td>10.74</td>\n",
       "      <td>4.76</td>\n",
       "      <td>9.44</td>\n",
       "      <td>10.11</td>\n",
       "      <td>14.77</td>\n",
       "      <td>6.73</td>\n",
       "      <td>8.42</td>\n",
       "      <td>11.11</td>\n",
       "      <td>14.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAG29340.1</td>\n",
       "      <td>13.42</td>\n",
       "      <td>14.10</td>\n",
       "      <td>14.20</td>\n",
       "      <td>100.00</td>\n",
       "      <td>61.76</td>\n",
       "      <td>13.04</td>\n",
       "      <td>18.39</td>\n",
       "      <td>17.24</td>\n",
       "      <td>14.94</td>\n",
       "      <td>...</td>\n",
       "      <td>9.43</td>\n",
       "      <td>9.57</td>\n",
       "      <td>8.65</td>\n",
       "      <td>7.46</td>\n",
       "      <td>5.30</td>\n",
       "      <td>11.32</td>\n",
       "      <td>7.76</td>\n",
       "      <td>6.12</td>\n",
       "      <td>5.62</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NP_057685.1</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.73</td>\n",
       "      <td>13.84</td>\n",
       "      <td>61.76</td>\n",
       "      <td>100.00</td>\n",
       "      <td>13.79</td>\n",
       "      <td>19.51</td>\n",
       "      <td>15.85</td>\n",
       "      <td>14.63</td>\n",
       "      <td>...</td>\n",
       "      <td>9.43</td>\n",
       "      <td>9.26</td>\n",
       "      <td>7.84</td>\n",
       "      <td>8.00</td>\n",
       "      <td>6.40</td>\n",
       "      <td>13.46</td>\n",
       "      <td>8.26</td>\n",
       "      <td>5.38</td>\n",
       "      <td>8.86</td>\n",
       "      <td>10.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>NP_269944.1</td>\n",
       "      <td>14.08</td>\n",
       "      <td>14.77</td>\n",
       "      <td>14.77</td>\n",
       "      <td>11.32</td>\n",
       "      <td>13.46</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.35</td>\n",
       "      <td>4.41</td>\n",
       "      <td>8.82</td>\n",
       "      <td>...</td>\n",
       "      <td>13.27</td>\n",
       "      <td>11.36</td>\n",
       "      <td>14.84</td>\n",
       "      <td>13.97</td>\n",
       "      <td>13.64</td>\n",
       "      <td>100.00</td>\n",
       "      <td>15.43</td>\n",
       "      <td>19.66</td>\n",
       "      <td>11.34</td>\n",
       "      <td>18.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>NP_003896.1</td>\n",
       "      <td>6.10</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.73</td>\n",
       "      <td>7.76</td>\n",
       "      <td>8.26</td>\n",
       "      <td>10.81</td>\n",
       "      <td>9.17</td>\n",
       "      <td>11.93</td>\n",
       "      <td>10.09</td>\n",
       "      <td>...</td>\n",
       "      <td>11.37</td>\n",
       "      <td>14.23</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.28</td>\n",
       "      <td>13.84</td>\n",
       "      <td>15.43</td>\n",
       "      <td>100.00</td>\n",
       "      <td>13.93</td>\n",
       "      <td>13.51</td>\n",
       "      <td>15.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>AAH10859.1</td>\n",
       "      <td>9.86</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.42</td>\n",
       "      <td>6.12</td>\n",
       "      <td>5.38</td>\n",
       "      <td>8.64</td>\n",
       "      <td>8.97</td>\n",
       "      <td>8.97</td>\n",
       "      <td>6.41</td>\n",
       "      <td>...</td>\n",
       "      <td>10.38</td>\n",
       "      <td>7.52</td>\n",
       "      <td>11.90</td>\n",
       "      <td>15.36</td>\n",
       "      <td>12.77</td>\n",
       "      <td>19.66</td>\n",
       "      <td>13.93</td>\n",
       "      <td>100.00</td>\n",
       "      <td>17.21</td>\n",
       "      <td>18.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>NP_004070.3</td>\n",
       "      <td>12.00</td>\n",
       "      <td>11.11</td>\n",
       "      <td>11.11</td>\n",
       "      <td>5.62</td>\n",
       "      <td>8.86</td>\n",
       "      <td>17.39</td>\n",
       "      <td>17.58</td>\n",
       "      <td>14.29</td>\n",
       "      <td>18.68</td>\n",
       "      <td>...</td>\n",
       "      <td>10.23</td>\n",
       "      <td>13.04</td>\n",
       "      <td>13.75</td>\n",
       "      <td>14.04</td>\n",
       "      <td>13.72</td>\n",
       "      <td>11.34</td>\n",
       "      <td>13.51</td>\n",
       "      <td>17.21</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>AAA25265.1</td>\n",
       "      <td>15.19</td>\n",
       "      <td>14.74</td>\n",
       "      <td>14.74</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>13.73</td>\n",
       "      <td>10.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>...</td>\n",
       "      <td>13.23</td>\n",
       "      <td>14.03</td>\n",
       "      <td>16.84</td>\n",
       "      <td>12.88</td>\n",
       "      <td>14.52</td>\n",
       "      <td>18.97</td>\n",
       "      <td>15.76</td>\n",
       "      <td>18.56</td>\n",
       "      <td>20.07</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows × 428 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       protein name  AAH65243.1  NP_937802.1  NP_001341533.1  AAG29340.1  \\\n",
       "0        AAH65243.1      100.00        97.34           97.37       13.42   \n",
       "1       NP_937802.1       97.34       100.00          100.00       14.10   \n",
       "2    NP_001341533.1       97.37       100.00          100.00       14.20   \n",
       "3        AAG29340.1       13.42        14.10           14.20      100.00   \n",
       "4       NP_057685.1       13.01        13.73           13.84       61.76   \n",
       "..              ...         ...          ...             ...         ...   \n",
       "422     NP_269944.1       14.08        14.77           14.77       11.32   \n",
       "423     NP_003896.1        6.10         6.73            6.73        7.76   \n",
       "424      AAH10859.1        9.86         8.42            8.42        6.12   \n",
       "425     NP_004070.3       12.00        11.11           11.11        5.62   \n",
       "426      AAA25265.1       15.19        14.74           14.74       10.20   \n",
       "\n",
       "     NP_057685.1  NP_000469.3  NP_001074551.1  AAI32679.1  NP_776412.2  ...  \\\n",
       "0          13.01        10.14           14.69       17.48        16.78  ...   \n",
       "1          13.73         9.74           14.09       16.78        16.11  ...   \n",
       "2          13.84         9.74           14.09       16.78        16.11  ...   \n",
       "3          61.76        13.04           18.39       17.24        14.94  ...   \n",
       "4         100.00        13.79           19.51       15.85        14.63  ...   \n",
       "..           ...          ...             ...         ...          ...  ...   \n",
       "422        13.46         7.25            7.35        4.41         8.82  ...   \n",
       "423         8.26        10.81            9.17       11.93        10.09  ...   \n",
       "424         5.38         8.64            8.97        8.97         6.41  ...   \n",
       "425         8.86        17.39           17.58       14.29        18.68  ...   \n",
       "426        10.59        13.73           10.00       11.00        11.00  ...   \n",
       "\n",
       "     pdb|1VRU|A  sp|Q9HAT2.1|SIAE_HUMAN  EAN77629.1  NP_644805.1  NP_009330.1  \\\n",
       "0         11.61                   11.21        5.21         9.09         8.33   \n",
       "1         11.30                   10.34        4.00         9.50        10.17   \n",
       "2         11.30                   10.74        4.76         9.44        10.11   \n",
       "3          9.43                    9.57        8.65         7.46         5.30   \n",
       "4          9.43                    9.26        7.84         8.00         6.40   \n",
       "..          ...                     ...         ...          ...          ...   \n",
       "422       13.27                   11.36       14.84        13.97        13.64   \n",
       "423       11.37                   14.23       15.27        16.28        13.84   \n",
       "424       10.38                    7.52       11.90        15.36        12.77   \n",
       "425       10.23                   13.04       13.75        14.04        13.72   \n",
       "426       13.23                   14.03       16.84        12.88        14.52   \n",
       "\n",
       "     NP_269944.1  NP_003896.1  AAH10859.1  NP_004070.3  AAA25265.1  \n",
       "0          14.08         6.10        9.86        12.00       15.19  \n",
       "1          14.77         6.73        8.42        11.11       14.74  \n",
       "2          14.77         6.73        8.42        11.11       14.74  \n",
       "3          11.32         7.76        6.12         5.62       10.20  \n",
       "4          13.46         8.26        5.38         8.86       10.59  \n",
       "..           ...          ...         ...          ...         ...  \n",
       "422       100.00        15.43       19.66        11.34       18.97  \n",
       "423        15.43       100.00       13.93        13.51       15.76  \n",
       "424        19.66        13.93      100.00        17.21       18.56  \n",
       "425        11.34        13.51       17.21       100.00       20.07  \n",
       "426        18.97        15.76       18.56        20.07      100.00  \n",
       "\n",
       "[427 rows x 428 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.4 Calculation of FoHs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have a dictionary of (cid: assays tested); (assay_tested:protein name), and percentage identity matrix with first columns as protein names. \n",
    "For each compound, we retrieve the list of all protein names tested on that compounds by matching between the two first dictionary. From this list, we retrieve the corresponding matrix of percentages identitiy of these proteins corresponding to these compounds and calculate the FoH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_si_dict = {}\n",
    "for name in protein_si['protein name']: \n",
    "    for other_name in protein_si['protein name']: \n",
    "        if other_name != name: \n",
    "            protein_si_dict[(name, other_name)] = protein_si.loc[protein_si['protein name'] == name, other_name].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/791 [00:00<00:32, 23.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 791/791 [02:02<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "foh_dict = {}\n",
    "\n",
    "for cid, targets in tqdm.tqdm(results_dict.items()):\n",
    "    active_weight_list = []\n",
    "    total_weight_list = []\n",
    "\n",
    "    for target_id, result in targets.items():\n",
    "        if target_id == '':\n",
    "            continue\n",
    "\n",
    "        protein_name = protein_id_to_name[target_id]\n",
    "        max_weight = 0\n",
    "\n",
    "        for other_id, other_result in targets.items():\n",
    "            if other_id != target_id and other_id != '':\n",
    "                other_protein_name = protein_id_to_name[other_id]\n",
    "                value = protein_si_dict[(protein_name, other_protein_name)]\n",
    "                max_weight = max(max_weight, value)\n",
    "\n",
    "        target_weight = 1 - max_weight / 100\n",
    "\n",
    "        if result:\n",
    "            active_weight_list.append(target_weight)\n",
    "        total_weight_list.append(target_weight)\n",
    "\n",
    "    if total_weight_list:\n",
    "        foh_score = sum(active_weight_list) / sum(total_weight_list)\n",
    "        foh_dict[cid] = foh_score\n",
    "    else: \n",
    "        foh_dict[cid] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export foh_dict\n",
    "with open(f'{data_folder}/before_finished/step_11/11_1/foh_dict.json', 'w') as f:\n",
    "    json.dump(foh_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compounds with FoH larger than 0.26, we remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 compounds with FoH larger than 0.26\n"
     ]
    }
   ],
   "source": [
    "to_drop = []\n",
    "for cid, foh_score in foh_dict.items():\n",
    "    if foh_score > 0.26: \n",
    "        to_drop.append(cid)\n",
    "\n",
    "post_FoH_hits = pre11_hits[~pre11_hits['PUBCHEM_CID'].isin(to_drop)]\n",
    "print(f'Dropped {(len(to_drop))} compounds with FoH larger than 0.26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save post_FoH_hits: \n",
    "post_FoH_hits.to_csv(f'{data_folder}/before_finished/step_11/11_1/post_FoH_hits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre11_inactives.to_csv(f'{data_folder}/before_finished/step_11/11_1/post_FoH_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Autofluoresence & Luceferase inhibition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finding false positive due to autofluorescence and luceferase inhibition, it is important to check if the particular assays use one of these technologies. Here, all three assays (AID626, AID1488, and AID1741) use fluorescence technologies, so it is optimal to remove compounds that are active in AIDs: 587, 588, 590, 591, 592, 593, 594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "autofluorescence_aids = ['587', '588', '590', '591', '592', '593', '594']\n",
    "autofluorescence_cids = []\n",
    "col_list = ['PUBCHEM_CID', 'PUBCHEM_ACTIVITY_OUTCOME']\n",
    "\n",
    "count = 0\n",
    "for AID in autofluorescence_aids:\n",
    "    url = f'https://pubchem.ncbi.nlm.nih.gov/assay/pcget.cgi?query=download&record_type=datatable&actvty=all&response_type=save&aid={AID}'\n",
    "    autofluorescence_df = pd.read_csv(url, usecols=col_list)\n",
    "    #delete rows with nan values\n",
    "    autofluorescence_df = autofluorescence_df.dropna(subset=['PUBCHEM_CID', 'PUBCHEM_ACTIVITY_OUTCOME'])\n",
    "\n",
    "    #convert cids to int: \n",
    "    autofluorescence_df['PUBCHEM_CID'] = autofluorescence_df['PUBCHEM_CID'].astype(int)\n",
    "\n",
    "    #keep only rows said \"Active\"\n",
    "    autofluorescence_df = autofluorescence_df[autofluorescence_df['PUBCHEM_ACTIVITY_OUTCOME'] == 'Active']\n",
    "    autofluorescence_cids.extend(autofluorescence_df['PUBCHEM_CID'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates in the list:\n",
    "autofluorescence_cids = [item for item in set(autofluorescence_cids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 autofluorescence compounds\n"
     ]
    }
   ],
   "source": [
    "to_drop_hits = []\n",
    "for cid in post_FoH_hits: \n",
    "    if cid in autofluorescence_cids:\n",
    "        to_drop.append(cid)\n",
    "post_autofluorescence = post_FoH_hits[~post_FoH_hits['PUBCHEM_CID'].isin(to_drop_hits)]\n",
    "print(f'Dropped {(len(to_drop_hits))} autofluorescence compounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{data_folder}/before_finished/step_11/11_2'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_11/11_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save: \n",
    "post_autofluorescence.to_csv(f'{data_folder}/before_finished/step_11/11_2/post_autofluorescence_hits.csv', index=False)\n",
    "pre11_inactives.to_csv(f'{data_folder}/before_finished/step_11/11_2/post_autofluorescence_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 RDKit PAIN filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = FilterCatalogParams()\n",
    "params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS)\n",
    "catalog = FilterCatalog(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pains detected\n",
      "2 pains detected\n",
      "3 pains detected\n",
      "4 pains detected\n",
      "5 pains detected\n",
      "6 pains detected\n",
      "7 pains detected\n",
      "8 pains detected\n",
      "9 pains detected\n",
      "10 pains detected\n",
      "11 pains detected\n",
      "12 pains detected\n",
      "13 pains detected\n",
      "14 pains detected\n",
      "15 pains detected\n",
      "16 pains detected\n",
      "17 pains detected\n",
      "18 pains detected\n",
      "19 pains detected\n",
      "20 pains detected\n",
      "21 pains detected\n",
      "22 pains detected\n",
      "23 pains detected\n",
      "24 pains detected\n",
      "25 pains detected\n",
      "26 pains detected\n",
      "27 pains detected\n",
      "28 pains detected\n"
     ]
    }
   ],
   "source": [
    "def detect_pains(df):\n",
    "    pains = []\n",
    "    count_pains = 0\n",
    "    count_not_pains = 0\n",
    "    smiles_column = 'PUBCHEM_EXT_DATASOURCE_SMILES'\n",
    "    cids_column = 'PUBCHEM_CID'\n",
    "    for i in df.index:\n",
    "        smile = str(df.loc[i, smiles_column])\n",
    "        cid = df.loc[i, cids_column]\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is not None:\n",
    "            if catalog.HasMatch(mol):\n",
    "                pains.append(cid)\n",
    "                count_pains += 1\n",
    "                print(f'{count_pains} pains detected')\n",
    "            else: \n",
    "                count_not_pains += 1\n",
    "    return pains\n",
    "\n",
    "pains = detect_pains(post_autofluorescence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pains_hits = post_autofluorescence[~post_autofluorescence['PUBCHEM_CID'].isin(pains)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{data_folder}/before_finished/step_11/11_3'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_11/11_3')\n",
    "\n",
    "post_pains_hits.to_csv(f'{data_folder}/before_finished/step_11/11_3/post_pains_hits.csv', index=False)\n",
    "pre11_inactives.to_csv(f'{data_folder}/before_finished/step_11/11_3/post_pains_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Drug-likeness filter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre12_hits = pd.read_csv(f'{data_folder}/before_finished/step_11/11_3/post_pains_hits.csv', sep=',', header=0)\n",
    "pre12_inactives = pd.read_csv(f'{data_folder}/before_finished/step_11/11_3/post_pains_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def drug_likeness_filter(smiles):\n",
    "    \"\"\"\n",
    "    This functions check if a given smiles satisfies the common standard conditions for drug-likeness. \n",
    "    Input:\n",
    "        SMILES (str)\n",
    "    Output: \n",
    "        Result (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert SMILES string to RDKit molecule object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return False  # Return False if the molecule cannot be parsed\n",
    "    \n",
    "    # Check molecular weight\n",
    "    mw = Chem.rdMolDescriptors.CalcExactMolWt(mol)\n",
    "    if not (150 < mw < 800):\n",
    "        return False\n",
    "    \n",
    "    # Check AlogP\n",
    "    logp = Chem.Crippen.MolLogP(mol)\n",
    "    if not (-0.3 < logp < 5):\n",
    "        return False\n",
    "    \n",
    "    # Check number of rotatable bonds\n",
    "    rotatable_bonds = Lipinski.NumRotatableBonds(mol)\n",
    "    if rotatable_bonds >= 15:\n",
    "        return False\n",
    "    \n",
    "    # Check H-bond acceptor count and H-bond donor count\n",
    "    hba = Lipinski.NumHAcceptors(mol)\n",
    "    hbd = Lipinski.NumHDonors(mol)\n",
    "    if hba >= 15 or hbd >= 15:\n",
    "        return False\n",
    "    \n",
    "    # Check total formal charge\n",
    "    total_charge = sum(atom.GetFormalCharge() for atom in mol.GetAtoms())\n",
    "    if not (-2 < total_charge < 2):\n",
    "        return False\n",
    "    \n",
    "    # If all filters passed, return True\n",
    "    return True\n",
    "\n",
    "def drug_likeness_filter_multiprocessing(df):\n",
    "    \"\"\"\n",
    "    This function update a given dataframe by dropping molecules that don't pass the drug-likeness filter.\n",
    "    \"\"\"\n",
    "    to_drop = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Create future tasks for each SMILES string in the dataframe\n",
    "        futures = {executor.submit(drug_likeness_filter, row['PUBCHEM_EXT_DATASOURCE_SMILES']): row['PUBCHEM_CID'] for index, row in df.iterrows()}\n",
    "        \n",
    "        # Use tqdm to display progress bar\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing SMILES\"):\n",
    "            cid = futures[future]\n",
    "            if not future.result():\n",
    "                to_drop.append(cid)\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SMILES: 100%|██████████| 763/763 [00:00<00:00, 1226150.94it/s]\n"
     ]
    }
   ],
   "source": [
    "not_drug_hits = drug_likeness_filter_multiprocessing(pre12_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SMILES: 100%|██████████| 100467/100467 [00:14<00:00, 7136.16it/s] \n"
     ]
    }
   ],
   "source": [
    "not_drug_inactives = drug_likeness_filter_multiprocessing(pre12_inactives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 111 hit compounds that do not pass the drug likeness filter\n",
      "Dropped 5423 inactive compounds that do not pass the drug likeness filter\n"
     ]
    }
   ],
   "source": [
    "post12_hits = pre12_hits[~pre12_hits['PUBCHEM_CID'].isin(not_drug_hits)]\n",
    "post12_inactives = pre12_inactives[~pre12_inactives['PUBCHEM_CID'].isin(not_drug_inactives)]\n",
    "\n",
    "print(f'Dropped {(len(not_drug_hits))} hit compounds that do not pass the drug likeness filter')\n",
    "print(f'Dropped {(len(not_drug_inactives))} inactive compounds that do not pass the drug likeness filter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{data_folder}/before_finished/step_12'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_12')\n",
    "\n",
    "#Export not_drug_hits and inactives:\n",
    "with open(f'{data_folder}/before_finished/step_12/not_drug_hits.json', 'w') as f:\n",
    "    json.dump(not_drug_hits, f)\n",
    "with open(f'{data_folder}/before_finished/step_12/not_drug_inactives.json', 'w') as f:\n",
    "    json.dump(not_drug_inactives, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save: \n",
    "post12_hits.to_csv(f'{data_folder}/before_finished/step_12/post12_hits.csv', index=False)\n",
    "post12_inactives.to_csv(f'{data_folder}/before_finished/step_12/post12_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. ChemBL Curation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre13_hits = pd.read_csv(f'{data_folder}/before_finished/step_12/post12_hits.csv', sep=',', header=0)\n",
    "pre13_inactives = pd.read_csv(f'{data_folder}/before_finished/step_12/post12_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_score(smiles, cid):\n",
    "    result = checker.check_molblock(Chem.MolToMolBlock(Chem.MolFromSmiles(smiles)))\n",
    "    if result == ():\n",
    "        penalty_score = 0\n",
    "    else:\n",
    "        penalty_score = result[0][0]\n",
    "    return cid, penalty_score\n",
    "\n",
    "def checker_multiprocessing(df):\n",
    "    chembl_score_dict = {}\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Map futures to CIDs directly for easier reference\n",
    "        futures = {executor.submit(checker_score, row['PUBCHEM_EXT_DATASOURCE_SMILES'], row['PUBCHEM_CID']): row['PUBCHEM_CID'] for _, row in df.iterrows()}\n",
    "        # Properly use tqdm to create a progress bar\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing SMILES\"):\n",
    "            cid, penalty_score = future.result()\n",
    "            chembl_score_dict[cid] = penalty_score\n",
    "    return chembl_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SMILES: 100%|██████████| 652/652 [00:01<00:00, 457.96it/s]\n",
      "[17:32:55] Conflicting single bond directions around double bond at index 7.\n",
      "[17:32:55]   BondStereo set to STEREONONE and single bond directions set to NONE.\n",
      "[17:33:17] Conflicting single bond directions around double bond at index 4.\n",
      "[17:33:17]   BondStereo set to STEREONONE and single bond directions set to NONE.\n",
      "Processing SMILES: 100%|██████████| 95044/95044 [00:29<00:00, 3178.91it/s]  \n"
     ]
    }
   ],
   "source": [
    "score_hits = checker_multiprocessing(pre13_hits)\n",
    "score_inactives = checker_multiprocessing(pre13_inactives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 2}\n",
      "{0, 2, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "#print all unique values in the dictionary:\n",
    "print(set(score_hits.values()))\n",
    "print(set(score_inactives.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new step folder\n",
    "if not os.path.exists(f'{data_folder}/before_finished/step_13'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_13')\n",
    "\n",
    "# save the scores:\n",
    "with open(f'{data_folder}/before_finished/step_13/score_hits.json', 'w') as f:\n",
    "    json.dump(score_hits, f)\n",
    "with open(f'{data_folder}/before_finished/step_13/score_inactives.json', 'w') as f:\n",
    "    json.dump(score_inactives, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all compounds with a penalty score of 7:\n",
    "to_drop_hits = []\n",
    "to_drop_inactives = []\n",
    "for cid, penalty_score in score_hits.items():\n",
    "    if penalty_score == 7:\n",
    "        to_drop_hits.append(cid)\n",
    "for cid, penalty_score in score_inactives.items():\n",
    "    if penalty_score == 7:\n",
    "        to_drop_inactives.append(cid)\n",
    "\n",
    "post13_hits = pre13_hits[~pre13_hits['PUBCHEM_CID'].isin(to_drop_hits)]\n",
    "post13_inactives = pre13_inactives[~pre13_inactives['PUBCHEM_CID'].isin(to_drop_inactives)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final_hits and inactives:\n",
    "post13_hits.to_csv(f'{data_folder}/before_finished/step_13/post13_hits.csv', index=False)\n",
    "post13_inactives.to_csv(f'{data_folder}/before_finished/step_13/post13_inactives.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Final handling of chemical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two problems that requires InChI update: \n",
    "\n",
    "(1) Some of the InChI will be missing, since the PubChem Identifier Exchange service might not able to find the corresponding InChI for the aromatized, neutralized SMILES. \n",
    "\n",
    "(2) While handling mixtures, some mixtures whose component molecules are identical will result in duplicates. Therefore, we need to check their activities. \n",
    "- If all duplicates share the same results (active/inactive), we keep one of them. \n",
    "- If duplicates of the same molecules returned different activity, we remove both of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import: \n",
    "pre14_hits = pd.read_csv(f'{data_folder}/before_finished/step_13/post13_hits.csv', sep=',', header=0)\n",
    "pre14_inactives = pd.read_csv(f'{data_folder}/before_finished/step_13/post13_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Update InChI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_to_inchi(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    inchi = Chem.inchi.MolToInchi(mol)\n",
    "    return inchi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 0 InChI values in pre14_hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:42:18] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:19] WARNING: Charges were rearranged\n",
      "\n",
      "[17:42:19] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:19] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:19] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:19] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:19] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:19] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:20] WARNING: Omitted undefined stereo\n",
      "\n",
      "[17:42:20] WARNING: Omitted undefined stereo\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 27 InChI values in pre14_inactives\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for index, row in pre14_hits.iterrows():\n",
    "    if row['InChI'] != row['InChI']:\n",
    "        pre14_hits.at[index, 'InChI'] = smi_to_inchi(row['PUBCHEM_EXT_DATASOURCE_SMILES'])\n",
    "        count += 1\n",
    "print(f'Updated {count} InChI values in pre14_hits')\n",
    "\n",
    "count = 0\n",
    "for index, row in pre14_inactives.iterrows():\n",
    "    if row['InChI'] != row['InChI']:\n",
    "        pre14_inactives.at[index, 'InChI'] = smi_to_inchi(row['PUBCHEM_EXT_DATASOURCE_SMILES'])\n",
    "        count += 1\n",
    "print(f'Updated {count} InChI values in pre14_inactives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC(C)(C)CCN1CCC[C@H](NC(=O)COc2ccc(F)c(Cl)c2)C1 SMILES appeared in both hit and inactive sets\n",
      "InChI=1S/C19H28ClFN2O2/c1-19(2,3)8-10-23-9-4-5-14(12-23)22-18(24)13-25-15-6-7-17(21)16(20)11-15/h6-7,11,14H,4-5,8-10,12-13H2,1-3H3,(H,22,24)/t14-/m0/s1 InChI appeared in both hit and inactive sets\n",
      "Number of InChI duplicates in hits:  0\n",
      "Number of InChI duplicates in inactives:  91\n",
      "Number of SMILES duplicates in hits:  0\n",
      "Number of SMILES duplicates in inactives:  91\n"
     ]
    }
   ],
   "source": [
    "#Check if a mol in hit set appeared in inactive set:\n",
    "for i in pre14_hits['PUBCHEM_EXT_DATASOURCE_SMILES']:\n",
    "    if i in list(pre14_inactives['PUBCHEM_EXT_DATASOURCE_SMILES']):\n",
    "        print(f'{i} SMILES appeared in both hit and inactive sets')\n",
    "for i in pre14_hits['InChI']:\n",
    "    if i in list(pre14_inactives['InChI']):\n",
    "        print(f'{i} InChI appeared in both hit and inactive sets')\n",
    "\n",
    "#Return all duplicates by comparing InChI:\n",
    "final_hits_duplicates_InChI = pre14_hits[pre14_hits.duplicated(subset=['InChI'], keep=False)]\n",
    "final_inactives_duplicates_InChI = pre14_inactives[pre14_inactives.duplicated(subset=['InChI'], keep=False)]\n",
    "final_hits_duplicates_smi = pre14_hits[pre14_hits.duplicated(subset=['PUBCHEM_EXT_DATASOURCE_SMILES'], keep=False)]\n",
    "final_inactives_duplicates_smi = pre14_inactives[pre14_inactives.duplicated(subset=['PUBCHEM_EXT_DATASOURCE_SMILES'], keep=False)]\n",
    "\n",
    "print('Number of InChI duplicates in hits: ', len(final_hits_duplicates_InChI))\n",
    "print('Number of InChI duplicates in inactives: ', len(final_inactives_duplicates_InChI))\n",
    "print('Number of SMILES duplicates in hits: ', len(final_hits_duplicates_smi))\n",
    "print('Number of SMILES duplicates in inactives: ', len(final_inactives_duplicates_smi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{data_folder}/before_finished/step_14'):\n",
    "    os.makedirs(f'{data_folder}/before_finished/step_14')\n",
    "\n",
    "#write all the duplicates to a file:\n",
    "#write duplicates to a txt file: \n",
    "with open(f'{data_folder}/before_finished/step_14/duplicates.txt', 'w') as f:\n",
    "    f.write('InChI duplicates in hits: \\n')\n",
    "    f.write(final_hits_duplicates_InChI.to_string())\n",
    "    f.write('\\n\\n')\n",
    "    f.write('InChI duplicates in inactives: \\n')\n",
    "    f.write(final_inactives_duplicates_InChI.to_string())\n",
    "    f.write('\\n\\n')\n",
    "    f.write('SMILES duplicates in hits: \\n')\n",
    "    f.write(final_hits_duplicates_smi.to_string())\n",
    "    f.write('\\n\\n')\n",
    "    f.write('SMILES duplicates in inactives: \\n')\n",
    "    f.write(final_inactives_duplicates_smi.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove these duplicates, keep the first one: \n",
    "#by inchi:\n",
    "final_hits = pre14_hits.drop_duplicates(subset=['InChI'], keep='first')\n",
    "final_inactives = pre14_inactives.drop_duplicates(subset=['InChI'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more duplicates in hits\n",
      "No more duplicates in inactives\n"
     ]
    }
   ],
   "source": [
    "if len(final_hits[final_hits.duplicated(subset=['PUBCHEM_EXT_DATASOURCE_SMILES'], keep=False)]) == 0:\n",
    "    print('No more duplicates in hits')\n",
    "\n",
    "if len(final_inactives[final_inactives.duplicated(subset=['PUBCHEM_EXT_DATASOURCE_SMILES'], keep=False)]) == 0:\n",
    "    print('No more duplicates in inactives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save: \n",
    "if not os.path.exists(f'{data_folder}/finished'):\n",
    "    os.makedirs(f'{data_folder}/finished')\n",
    "final_hits.to_csv(f'{data_folder}/finished/final_hits.csv',sep=',', index=False)\n",
    "final_inactives.to_csv(f'{data_folder}/finished/final_inactives.csv',sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1. Adjust columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hits = pd.read_csv(f'{data_folder}/finished/final_hits.csv', sep=',', header=0)\n",
    "final_inactives = pd.read_csv(f'{data_folder}/finished/final_inactives.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another column: \"activity_value\" with all empty NaN values: \n",
    "final_hits.loc[:, 'activity_value'] = np.nan\n",
    "final_inactives.loc[:, 'activity_value'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns:\n",
    "final_hits = final_hits.rename(columns={\n",
    "    'PUBCHEM_CID': 'CID', \n",
    "    'PUBCHEM_ACTIVITY_OUTCOME': 'activity_outcome',\n",
    "    'PUBCHEM_EXT_DATASOURCE_SMILES': 'SMILES',\n",
    "    'Mol removed from mixture': 'mol_removed_from_mixture',\n",
    "    'Small inorganic molecule': 'small_inorganic_mol_from_mixture',\n",
    "    'Small organic molecule': 'small_organic_mol_from_mixture'\n",
    "})\n",
    "final_inactives = final_inactives.rename(columns={\n",
    "    'PUBCHEM_CID': 'CID', \n",
    "    'PUBCHEM_ACTIVITY_OUTCOME': 'activity_outcome',\n",
    "    'PUBCHEM_EXT_DATASOURCE_SMILES': 'SMILES',\n",
    "    'Mol removed from mixture': 'mol_removed_from_mixture',\n",
    "    'Small inorganic molecule': 'small_inorganic_mol_from_mixture',\n",
    "    'Small organic molecule': 'small_organic_mol_from_mixture'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no \"small_organic_mol_from_mixture\" column, add it: \n",
    "if 'small_organic_mol_from_mixture' not in final_hits.columns:\n",
    "    final_hits.loc[:, 'small_organic_mol_from_mixture'] = np.nan\n",
    "if 'small_organic_mol_from_mixture' not in final_inactives.columns:\n",
    "    final_inactives.loc[:, 'small_organic_mol_from_mixture'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap the positions of the columns InChI and activity_outcome: \n",
    "final_hits = final_hits[['CID', 'SMILES', 'InChI', 'activity_outcome', 'activity_value', 'mol_removed_from_mixture', 'small_inorganic_mol_from_mixture', 'small_organic_mol_from_mixture']]\n",
    "final_inactives = final_inactives[['CID', 'SMILES', 'InChI', 'activity_outcome', 'activity_value', 'mol_removed_from_mixture', 'small_inorganic_mol_from_mixture', 'small_organic_mol_from_mixture']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export: \n",
    "final_hits.to_csv(f'{data_folder}/finished/final_hits.csv', sep=',', index=False)\n",
    "final_inactives.to_csv(f'{data_folder}/finished/final_inactives.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2. Exporting raw data without further curation (only smiles, inchi) for control experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for AID in AIDs: \n",
    "    exec(f\"raw{AID} = pd.read_csv(f'{data_folder}/before_finished/step_1/AID{AID}.csv', sep=',', header=0)\")\n",
    "\n",
    "#import inchi:\n",
    "for AID in AIDs: \n",
    "    exec(f\"std_inchi{AID} = pd.read_csv(f'{data_folder}/before_finished/step_3/std_inchi_{AID}.txt', sep='\\t', header=None)\")\n",
    "\n",
    "#Update inchi\n",
    "for AID in AIDs: \n",
    "    exec(f\"\"\"\n",
    "raw_inchi_dict{AID} = dict(zip(std_inchi{AID}[0], std_inchi{AID}[1]))\n",
    "raw{AID}['InChI'] = raw{AID}['PUBCHEM_CID'].map(raw_inchi_dict{AID})\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hits = raw449739[raw449739['PUBCHEM_ACTIVITY_OUTCOME'] == 'Active']\n",
    "raw_inactives = raw449739[raw449739['PUBCHEM_ACTIVITY_OUTCOME'] == 'Inactive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some other columns to match the format of the curated data: \n",
    "raw_hits.loc[:, 'activity_value'] = np.nan\n",
    "raw_hits.loc[:, 'mol_removed_from_mixture'] = np.nan\n",
    "raw_hits.loc[:, 'small_inorganic_mol_from_mixture'] = np.nan\n",
    "raw_hits.loc[:, 'small_organic_mol_from_mixture'] = np.nan\n",
    "\n",
    "raw_inactives.loc[:, 'activity_value'] = np.nan\n",
    "raw_inactives.loc[:, 'mol_removed_from_mixture'] = np.nan\n",
    "raw_inactives.loc[:, 'small_inorganic_mol_from_mixture'] = np.nan\n",
    "raw_inactives.loc[:, 'small_organic_mol_from_mixture'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns:\n",
    "raw_hits = raw_hits.rename(columns={\n",
    "    'PUBCHEM_CID': 'CID', \n",
    "    'PUBCHEM_ACTIVITY_OUTCOME': 'activity_outcome',\n",
    "    'PUBCHEM_EXT_DATASOURCE_SMILES': 'SMILES',\n",
    "})\n",
    "raw_inactives = raw_inactives.rename(columns={\n",
    "    'PUBCHEM_CID': 'CID', \n",
    "    'PUBCHEM_ACTIVITY_OUTCOME': 'activity_outcome',\n",
    "    'PUBCHEM_EXT_DATASOURCE_SMILES': 'SMILES',\n",
    "})\n",
    "\n",
    "#swap the positions of the columns InChI and activity_outcome: \n",
    "raw_hits = raw_hits[['CID', 'SMILES', 'InChI', 'activity_outcome', 'activity_value', 'mol_removed_from_mixture', 'small_inorganic_mol_from_mixture', 'small_organic_mol_from_mixture']]\n",
    "raw_inactives = raw_inactives[['CID', 'SMILES', 'InChI', 'activity_outcome', 'activity_value', 'mol_removed_from_mixture', 'small_inorganic_mol_from_mixture', 'small_organic_mol_from_mixture']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{data_folder}/finished/control_data'):\n",
    "    os.makedirs(f'{data_folder}/finished/control_data')\n",
    "\n",
    "#save the hits and inactives\n",
    "raw_hits.to_csv(f'{data_folder}/finished/control_data/raw_hits.csv', sep=',', index=False)\n",
    "raw_inactives.to_csv(f'{data_folder}/finished/control_data/raw_inactives.csv', sep=',', index=False)\n",
    "\n",
    "#save as txt: \n",
    "raw_hits.to_csv(f'{data_folder}/finished/control_data/raw_hits.txt', sep=';', index=False, header=False)\n",
    "raw_inactives.to_csv(f'{data_folder}/finished/control_data/raw_inactives.txt', sep=';', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.3. Convert the files to txt. for CORINA Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hits = pd.read_csv(f'{data_folder}/finished/final_hits.csv', sep=',', header=0)\n",
    "final_inactives = pd.read_csv(f'{data_folder}/finished/final_inactives.csv', sep=',', header=0)\n",
    "\n",
    "# export to .txt files:\n",
    "final_hits.to_csv(f'{data_folder}/finished/final_hits.txt', sep=';', index=False, header=False)\n",
    "final_inactives.to_csv(f'{data_folder}/finished/final_inactives.txt', sep=';', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
